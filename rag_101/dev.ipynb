{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a development notebook, not meant for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "from unstructured.cleaners.core import clean_extra_whitespace, group_broken_paragraphs\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    UnstructuredFileLoader(\n",
    "        \"/teamspace/studios/this_studio/example_data/2401.08406.pdf\",\n",
    "        post_processors=[clean_extra_whitespace, group_broken_paragraphs],\n",
    "    ),\n",
    "    UnstructuredFileLoader(\n",
    "        \"/teamspace/studios/this_studio/example_data/2401.00908.pdf\",\n",
    "        post_processors=[clean_extra_whitespace, group_broken_paragraphs],\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\\n\", \"\\n\\n\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=300,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(\n",
    "        loader.load_and_split(text_splitter=text_splitter),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}  # set True to compute cosine similarity\n",
    "\n",
    "embeddings_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(documents=docs, embedding=embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(results, *scores):\n",
    "    if scores:\n",
    "        results = zip(results, scores)\n",
    "\n",
    "    for result in results:\n",
    "        if isinstance(result, tuple):\n",
    "            print(result[1])\n",
    "            print(result[0])\n",
    "        else:\n",
    "            print(result)\n",
    "        print(\"\\n---------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"Where was the agriculture dataset collected for the USA?\"\n",
    "query2 = \"How many pdf data were collected from the USA?\"\n",
    "query3 = \"Which contries were used to collect dataset?\"\n",
    "query4 = \"What are the metrics used to evaluate the answers?\"\n",
    "query5 = \"how was the content and structure of available documents augmented?\"\n",
    "query6 = \"What was the answer generation process used in the paper?\"\n",
    "query7 = \"How many pdf data were collected from the USA ?\"\n",
    "query8 = \"What is the DocLLM architecture ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'q' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m queries \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     query1,\n\u001b[1;32m      3\u001b[0m     query2,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     query8,\n\u001b[1;32m      9\u001b[0m ]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, query \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(queries):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Query->\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mq\u001b[49m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     15\u001b[0m     )\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrieved document:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'q' is not defined"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    query1,\n",
    "    query2,\n",
    "    query3,\n",
    "    query4,\n",
    "    query5,\n",
    "    query7,\n",
    "    query8,\n",
    "]\n",
    "\n",
    "for i, query in enumerate(queries):\n",
    "    print(f\"Example {i+1}: Query->\", query)\n",
    "    print(\n",
    "        \"..\" * 50,\n",
    "    )\n",
    "    print(\"Retrieved document:\")\n",
    "\n",
    "    retrieved_documents = retriever.get_relevant_documents(query)\n",
    "    reranked_documents = rerank_docs(reranker_model, query, retrieved_documents)\n",
    "\n",
    "    print(\"--\" * 50)\n",
    "    print(reranked_documents[0][0].page_content)\n",
    "    print(\"--\" * 50)\n",
    "    print(\"metadata:\", reranked_documents[0][0].metadata)\n",
    "    print(\"==\" * 50, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Where was the agriculture dataset collected for the USA?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Where was the agriculture dataset collected for the USA?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6560989752855699</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m0.6560989752855699\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'3 Dataset Overview\\n\\nThis study evaluates fine-tuned and RAG-enhanced language models using </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">context-related questions and answers originated datasets from three major crop producer countries: USA, Brazil and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">India. In our case, we are using agriculture as the industrial setting as an example. Available data varied </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">considerably in format and content, ranging from regulatory documents, scientific reports, agronomic exams, to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge databases. In this section, we present each dataset in more detail.\\n\\n3.1 USA'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/teamspace/studios/this_studio/example_data/2401.08406.pdf'</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m'3 Dataset Overview\\n\\nThis study evaluates fine-tuned and RAG-enhanced language models using \u001b[0m\n",
       "\u001b[32mcontext-related questions and answers originated datasets from three major crop producer countries: USA, Brazil and\u001b[0m\n",
       "\u001b[32mIndia. In our case, we are using agriculture as the industrial setting as an example. Available data varied \u001b[0m\n",
       "\u001b[32mconsiderably in format and content, ranging from regulatory documents, scientific reports, agronomic exams, to \u001b[0m\n",
       "\u001b[32mknowledge databases. In this section, we present each dataset in more detail.\\n\\n3.1 USA'\u001b[0m,\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'/teamspace/studios/this_studio/example_data/2401.08406.pdf'\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "---------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "---------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = db.similarity_search_with_relevance_scores(query1, k=1)\n",
    "print(query1)\n",
    "pretty_print_docs(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "reranker_model = CrossEncoder(model_name=\"BAAI/bge-reranker-large\", max_length=512)\n",
    "\n",
    "\n",
    "def rerank_docs(query, retrieved_docs):\n",
    "    query_and_docs = [(query, r.page_content) for r in retrieved_docs]\n",
    "    scores = reranker_model.predict(query_and_docs)\n",
    "    return sorted(list(zip(retrieved_docs, scores)), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What is the DocLLM architecture ?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "What is the DocLLM architecture ?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = query8\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'4.2 Model Setup and Training Details\\n\\nTable 4 provides key settings and hyperparameters for two</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">variants of DocLLM: DocLLM-1B, which is based on the Falcon-1B architecture [5], and DocLLM-7B, which is based on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the Llama2-7B architecture [4]3. DocLLM-1B is composed of 24 layers, each with 16 attention heads and a hidden size</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of 1,536. DocLLM-7B comprises 36 layers, 32 heads, and a hidden size of 4,096. Using pre-trained weights as the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">backbone for the text modality, we extend the Falcon-1B and Llama2-7B models by adding the disentangled attention </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and block infilling objective as described in Section 3. For DocLLM-1B, we use a pre-training learning rate of 2 × </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">10−4 with 1,000 warmup steps, employing a cosine scheduler, and Adam optimizer [58] with β1 = 0.9, β2 = 0.96 and a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">weight decay of 0.1. For instruction tuning we use a learning\\n\\n3Since Llama2 does not come with pre-trained </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">weights at 1B parameters, we use the Falcon-1B architecture for the smaller\\n\\nversion of DocLLM.\\n\\n7'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/teamspace/studios/this_studio/example_data/2401.00908.pdf'</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m'4.2 Model Setup and Training Details\\n\\nTable 4 provides key settings and hyperparameters for two\u001b[0m\n",
       "\u001b[32mvariants of DocLLM: DocLLM-1B, which is based on the Falcon-1B architecture \u001b[0m\u001b[32m[\u001b[0m\u001b[32m5\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, and DocLLM-7B, which is based on \u001b[0m\n",
       "\u001b[32mthe Llama2-7B architecture \u001b[0m\u001b[32m[\u001b[0m\u001b[32m4\u001b[0m\u001b[32m]\u001b[0m\u001b[32m3. DocLLM-1B is composed of 24 layers, each with 16 attention heads and a hidden size\u001b[0m\n",
       "\u001b[32mof 1,536. DocLLM-7B comprises 36 layers, 32 heads, and a hidden size of 4,096. Using pre-trained weights as the \u001b[0m\n",
       "\u001b[32mbackbone for the text modality, we extend the Falcon-1B and Llama2-7B models by adding the disentangled attention \u001b[0m\n",
       "\u001b[32mand block infilling objective as described in Section 3. For DocLLM-1B, we use a pre-training learning rate of 2 × \u001b[0m\n",
       "\u001b[32m10−4 with 1,000 warmup steps, employing a cosine scheduler, and Adam optimizer \u001b[0m\u001b[32m[\u001b[0m\u001b[32m58\u001b[0m\u001b[32m]\u001b[0m\u001b[32m with β1 = 0.9, β2 = 0.96 and a \u001b[0m\n",
       "\u001b[32mweight decay of 0.1. For instruction tuning we use a learning\\n\\n3Since Llama2 does not come with pre-trained \u001b[0m\n",
       "\u001b[32mweights at 1B parameters, we use the Falcon-1B architecture for the smaller\\n\\nversion of DocLLM.\\n\\n7'\u001b[0m,\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'/teamspace/studios/this_studio/example_data/2401.00908.pdf'\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = db.similarity_search_with_score(query, k=1)\n",
    "for r in results:\n",
    "    print(r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\\n\\nDocLLM is constructed upon the foundation of an auto-regressive transformer language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model [4] following a causal decoder structure. It is composed of stacked transformer blocks, where each block </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">contains a multi-head self-attention layer and a fully connected feed forward network. Standard language models are</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">typically unimodal, accepting only a sequence of text tokens as input. In contrast, DocLLM is a multi-modal system </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">obtained using OCR. Simply augmenting the text with bounding box information via additive positional encoding may </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">not capture the intricate relationships between text semantics and spatial layout, especially for visually rich </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">documents [10]. Consequently, we treat the spatial information about the text tokens as a distinct modality. In </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">particular, we use separate vectors to represent these two modalities and extend the self-attention mechanism of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the transformer architecture to compute their inter-dependencies in a disentangled manner, as explained in the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">following section. Furthermore, instead of the traditional left-to-right next token prediction during </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">self-supervised training, we employ a text infilling objective that better leverages contextual information.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/teamspace/studios/this_studio/example_data/2401.00908.pdf'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9895678</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'\\n\\nDocLLM is constructed upon the foundation of an auto-regressive transformer language \u001b[0m\n",
       "\u001b[32mmodel \u001b[0m\u001b[32m[\u001b[0m\u001b[32m4\u001b[0m\u001b[32m]\u001b[0m\u001b[32m following a causal decoder structure. It is composed of stacked transformer blocks, where each block \u001b[0m\n",
       "\u001b[32mcontains a multi-head self-attention layer and a fully connected feed forward network. Standard language models are\u001b[0m\n",
       "\u001b[32mtypically unimodal, accepting only a sequence of text tokens as input. In contrast, DocLLM is a multi-modal system \u001b[0m\n",
       "\u001b[32mthat integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens \u001b[0m\n",
       "\u001b[32mobtained using OCR. Simply augmenting the text with bounding box information via additive positional encoding may \u001b[0m\n",
       "\u001b[32mnot capture the intricate relationships between text semantics and spatial layout, especially for visually rich \u001b[0m\n",
       "\u001b[32mdocuments \u001b[0m\u001b[32m[\u001b[0m\u001b[32m10\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. Consequently, we treat the spatial information about the text tokens as a distinct modality. In \u001b[0m\n",
       "\u001b[32mparticular, we use separate vectors to represent these two modalities and extend the self-attention mechanism of \u001b[0m\n",
       "\u001b[32mthe transformer architecture to compute their inter-dependencies in a disentangled manner, as explained in the \u001b[0m\n",
       "\u001b[32mfollowing section. Furthermore, instead of the traditional left-to-right next token prediction during \u001b[0m\n",
       "\u001b[32mself-supervised training, we employ a text infilling objective that better leverages contextual information.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'/teamspace/studios/this_studio/example_data/2401.00908.pdf'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;36m0.9895678\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retrieved_documents = retriever.get_relevant_documents(query)\n",
    "reranked_documents = rerank_docs(query, retrieved_documents)\n",
    "print(reranked_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
