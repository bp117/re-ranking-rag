{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG: Data Ingestion and Retrieval for complex documents\n",
    "\n",
    "\n",
    "<img src=\"arch.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process document and create vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rag_101.retriever import (\n",
    "    load_pdf,\n",
    "    split_text,\n",
    "    load_embedding_model,\n",
    "    load_reranker_model,\n",
    "    generate_embeddings,\n",
    "    create_compression_retriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files = [\"example_data/2401.08406.pdf\", \"example_data/2401.00908.pdf\"]  # Use your document/s\n",
    "loaders = load_pdf(files=files)\n",
    "documents = split_text(loaders=loaders, chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /teamspace/studios/this_studio/weights/sentence_transformers/BAAI_bge-reranker-large. Creating a new one with MEAN pooling.\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at /teamspace/studios/this_studio/weights/sentence_transformers/BAAI_bge-reranker-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings... This might take some time.\n"
     ]
    }
   ],
   "source": [
    "embedding_model = load_embedding_model(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "reranker_model = load_reranker_model(reranker_model_name=\"BAAI/bge-reranker-large\")\n",
    "\n",
    "print(\"Generating embeddings... This might take some time.\")\n",
    "vectorstore = generate_embeddings(documents, embedding_model=embedding_model)\n",
    "\n",
    "compression_retriever = create_compression_retriever(\n",
    "    base_retriever=vectorstore.as_retriever(search_kwargs={\"k\": 4}),\n",
    "    reranker_model=reranker_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the input document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User query: What are the metrics used to evaluate the answers? \n",
      "\n",
      "Retrieved content:\n",
      " LLMs as judges for open-ended scenarios (Zhu et al., 2023). In this work, we have used the AzureML Model Evaluation (Microsoft, 2023), with the following metrics to compare the generated answers with ground truth:\n",
      "\n",
      "Coherence: Comparison of coherence between ground truths and predictions given the context. The metric provides a score between one to five, where one means that the answer lacks coherence and five means the answer has perfect coherency. An example is provided in Table 6.\n",
      "\n",
      "Relevance: Relevance measures how well the answer addresses the main aspects of the question based on the context. The metric rates from 1 to 5, where 5 means the answer has perfect relevance. An example is provided in Table 7.\n",
      "\n",
      "Groundedness: The metric defines weather the answer follows logically from the information contained in the context or not and provides and integer score to determine how grounded the answer is. An example is provided in Table 8.\n",
      "\n",
      "17\n",
      "\n",
      "Question\n",
      "\n",
      "Relevance Score â†‘\n",
      "\n",
      "Answer\n",
      "metadata: {'source': 'example_data/2401.08406.pdf'}\n",
      "-------------------------------------------------- \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the metrics used to evaluate the answers?\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(query)\n",
    "print(\"User query:\", query, \"\\n\")\n",
    "print(\"Retrieved content:\\n\", compressed_docs[0].page_content)\n",
    "print(\"metadata:\", compressed_docs[0].metadata)\n",
    "print(\"-\" * 50, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User query: What is the DocLLM architecture ? \n",
      "\n",
      "Retrieved content:\n",
      " The maximum sequence length, or context length, is consistently set to 1,024 for both versions during the entire training process. The DocLLM-7B models are trained with 16-bit mixed precision on 8 24GB A10g GPUs using fully sharded data parallelism, implemented with the accelerate library.4 The DocLLM-1B model, on the other hand, is trained on a single 24GB A10g GPU.\n",
      "\n",
      "4.3 Downstream Evaluation\n",
      "\n",
      "Experimental settings. We investigate two experimental settings:\n",
      "\n",
      "Same Datasets, Different Splits (SDDS): Following previous work in VRDU [34, 59, 33, 12, 31, 32], we first evaluate DocLLM on the unseen test split (or dev split when test split is unavailable) of each of the 16 datasets composing the instruction-tuning data. The motivation behind this very typical setting is to check how DocLLM performs when tasks and domains supposedly stay the same from train to test.\n",
      "metadata: {'source': 'example_data/2401.00908.pdf'}\n",
      "-------------------------------------------------- \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the DocLLM architecture ?\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(query)\n",
    "print(\"User query:\", query, \"\\n\")\n",
    "print(\"Retrieved content:\\n\", compressed_docs[0].page_content)\n",
    "print(\"metadata:\", compressed_docs[0].metadata)\n",
    "print(\"-\" * 50, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Query-> How many pdf data were collected from the USA?\n",
      "....................................................................................................\n",
      "Retrieved document:\n",
      "3.1 USA\n",
      "\n",
      "We collected documents, handbooks, and reports publicly available online from the United States Department of Agriculture (USDA), state agriculture and consumer services agencies, and partners from the Land-Grant Institutions National Program. Available documents contain federal regulatory and policy information surrounding crop and livestock management, information on diseases and best practices, quality assurance and export regulations, details on assistance programs, as well as insurance and pricing guidelines. Collected data totals more than 23k PDF files with over 50M tokens, representing 44 states in the USA. We downloaded and preprocessed these files, extracting the textual information that could be used as input to the Q&A generation pipeline. To benchmark and evaluate the models, we employed the documents related to the Washington state, which comprises 573 files with over 2M tokens. We present in Listing 5 an example of content within these documents.\n",
      "metadata: {'source': 'example_data/2401.08406.pdf'}\n",
      "==================================================================================================== \n",
      "\n",
      "\n",
      "Example 2: Query-> Which contries were used to collect dataset?\n",
      "....................................................................................................\n",
      "Retrieved document:\n",
      "Listing 7 an example question and answer from the India dataset. The answers from KVK are brief and consist of at most one sentence. Using the Vikaspedia data, we enhanced the answers from KVK to make them more detailed and clear for the farmers.\n",
      "\n",
      "4 Metrics\n",
      "\n",
      "The primary purpose of this section is to establish a comprehensive set of metrics designed to guide the quality assessment of the Q&A generation process, particularly for the fine-tuning and RAG methodologies. A fundamental principle in machine learning is that the quality of the model is strongly influenced by the quality of the dataset it is trained on. In this context, to train or fine-tune a model that effectively generates high-quality Q&A pairs, we first need a robust dataset. The inputs of our framework is the Q&A pairs along with their associated context, and the output a quantification the quality of these pairs and, finally, of the fine-tuned model. The quality here is determined on a set of parameters.\n",
      "metadata: {'source': 'example_data/2401.08406.pdf'}\n",
      "==================================================================================================== \n",
      "\n",
      "\n",
      "Example 3: Query-> Where was the agriculture dataset collected for the USA?\n",
      "....................................................................................................\n",
      "Retrieved document:\n",
      "acres, forced U.S. apple growers to also market these varieties domestically.\n",
      "\n",
      "...\n",
      "\n",
      "Listing 5: Example of a passage in one of the Washington state documents, focusing on the production of Gala apples.\n",
      "\n",
      "3.2 Brazil\n",
      "\n",
      "We used a dataset called \"500 Questions 500 Answers - Embrapa/SCT\" containing books with questions and answers related to various aspects of crop cultivation and management in Brazil. The questions are text-based inquiries formulated by a diverse group of stakeholders, including producers, farmers, and farming associations, and are accompanied by responses from Embrapa specialists.\n",
      "metadata: {'source': 'example_data/2401.08406.pdf'}\n",
      "==================================================================================================== \n",
      "\n",
      "\n",
      "Example 4: Query-> What was the answer generation process used in the paper?\n",
      "....................................................................................................\n",
      "Retrieved document:\n",
      "In terms of Relevance, the combined generation method outperforms the only questions method in all contexts. This suggests that generating both questions and answers produces content that is more relevant to the context, when considering a farmer perspective. On the other hand, we tested changing the prompt used to calculate Relevance and removed the farmer perspective. With that, the score of both setups was extremely similar. Lastly, for Fluency, which measures the readability and naturalness of the generated content, both methods perform similarly across all contexts, indicating that both methods generate highly fluent content.\n",
      "metadata: {'source': 'example_data/2401.08406.pdf'}\n",
      "==================================================================================================== \n",
      "\n",
      "\n",
      "Example 5: Query-> how was the content and structure of available documents augmented?\n",
      "....................................................................................................\n",
      "Retrieved document:\n",
      "First, we augment the content and structure of available documents by explicitly adding supporting tags from the text. We formulated prompts to extract a list of locations and agronomic topics mentioned in each section of the document (e.g., if that section refers to crops, cattle, or diseases), as exemplified in Listing 3, and task the LLM model to answering them based on the data extracted from the JSON files. The aim is to use of the the additional information, including locations and mentioned topics, to ground the generation process, enhancing the relevance of the questions and guiding the model to cover a wide range of topics and challenges.\n",
      "\n",
      "7\n",
      "\n",
      "{\n",
      "\n",
      "\"grobid_version\": \"0.7.3\", \"grobid_timestamp\": \"2023-07-04T13:05+0000\", \"language_code\": \"en\", \"citations\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"authors\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"given_name\": \"J\", \"surname\": \"Abatzoglou\", \"name\": \"J T Abatzoglou\"\n",
      "\n",
      "}, {\n",
      "\n",
      "\"given_name\": \"T\", \"surname\": \"Brown\", \"name\": \"T J Brown\"\n",
      "\n",
      "}\n",
      "metadata: {'source': 'example_data/2401.08406.pdf'}\n",
      "==================================================================================================== \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run through some sample queries\n",
    "\n",
    "query1 = \"What are the metrics used to evaluate the answers?\"\n",
    "query2 = \"How many pdf data were collected from the USA?\"\n",
    "query3 = \"What is the DocLLM architecture ?\"\n",
    "query4 = \"Which contries were used to collect dataset?\"\n",
    "query5 = \"Where was the agriculture dataset collected for the USA?\"\n",
    "query6 = \"how was the content and structure of available documents augmented?\"\n",
    "query7 = \"What was the answer generation process used in the paper?\"\n",
    "query8 = \"how was the content and structure of available documents augmented?\"\n",
    "\n",
    "queries = [\n",
    "    # query1,\n",
    "    query2,\n",
    "    # query3,\n",
    "    query4,\n",
    "    query5,\n",
    "    query7,\n",
    "    query8,\n",
    "]\n",
    "\n",
    "for i, q in enumerate(queries):\n",
    "    print(f\"Example {i+1}: Query->\", q)\n",
    "    print(\n",
    "        \"..\" * 50,\n",
    "    )\n",
    "    print(\"Retrieved document:\")\n",
    "    compressed_docs = compression_retriever.get_relevant_documents(q)\n",
    "    print(f\"{compressed_docs[0].page_content}\")\n",
    "    print(\"metadata:\", compressed_docs[0].metadata)\n",
    "    print(\"==\" * 50, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
