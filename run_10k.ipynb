{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Search and Retrieval for complex documents using RAG\n",
    "\n",
    "\n",
    "<img src=\"arch.png\" width=400px>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process document and create vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rag_101.retriever import (\n",
    "    load_pdf,\n",
    "    split_text,\n",
    "    load_embedding_model,\n",
    "    load_reranker_model,\n",
    "    generate_embeddings,\n",
    "    rerank_docs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load documents and split into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-yl1usx3d because the default path (/teamspace/studios/this_studio/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "# Load two PDFs from arxiv\n",
    "# You can upload your own document and update the path\n",
    "files = [\"10k_docs/10k.pdf\", \"10k_docs/amazon-10k.pdf\"]\n",
    "loaders = load_pdf(files=files)\n",
    "\n",
    "# Spliting\n",
    "documents = split_text(loaders=loaders, chunk_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate embeddings and store in a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273978582a5b47bba456885d07ba8858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f78b36ebab84dd4ae6e02c92b0b7f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9411a0b34a4b8ea0a35f5d4c09bd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7171d132a34b51816b33c8fa87ed60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f871f405a04baa8e454f6578213346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee48784b3544e0788bd0797d643c381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings... This might take some time.\n"
     ]
    }
   ],
   "source": [
    "# initialize models\n",
    "embedding_model = load_embedding_model(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "reranker_model = load_reranker_model(reranker_model_name=\"BAAI/bge-reranker-large\")\n",
    "\n",
    "# generate embeddings and store in vector database\n",
    "print(\"Generating embeddings... This might take some time.\")\n",
    "vectorstore = generate_embeddings(documents, embedding_model=embedding_model)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the input document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User query: What is the DocLLM architecture ?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Retrieved content:\n",
      "\n",
      "\n",
      "DocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure. It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network. Standard language models are typically unimodal, accepting only a sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained using OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout, especially for visually rich documents [10]. Consequently, we treat the spatial information about the text tokens as a distinct modality. In particular, we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner, as explained in the following section. Furthermore, instead of the traditional left-to-right next token prediction during self-supervised training, we employ a text infilling objective that better leverages contextual information.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "metadata: {'source': 'example_data/2401.00908.pdf'}\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the earning of Microsoft ?\"\n",
    "\n",
    "retrieved_documents = retriever.get_relevant_documents(query)\n",
    "reranked_documents = rerank_docs(reranker_model, query, retrieved_documents)\n",
    "\n",
    "print(\"\\nUser query:\", query)\n",
    "print(\"--\" * 50)\n",
    "print(\n",
    "    \"Retrieved content:\",\n",
    ")\n",
    "print(reranked_documents[0][0].page_content)\n",
    "print(\"--\" * 50)\n",
    "print(\"metadata:\", reranked_documents[0][0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run through some sample queries and observe the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Query-> What are the metrics used to evaluate the answers?\n",
      "....................................................................................................\n",
      "Retrieved document:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "10 [Document(page_content='\\n\\nIn the existing literature, it’s common to find a wealth of metrics designed to evaluate the quality of answers. These metrics play an indispensable role in ensuring that the answers provided by models are accurate, relevant, and effectively address the questions posed. However, there is a notable gap when it comes to metrics specifically designed to assess the quality of the questions themselves. Recognizing this lack, we have developed our metrics with a focus on evaluating question quality. Given the pivotal role that questions play in driving meaningful conversations and generating useful answers, it is equally important to ensure their quality as it is for the answers. This quantitative evaluation of the Q&A sets will not only help in refining the generation process but also ensure the relevance and accuracy of the output, ultimately enhancing the overall efficacy of our model. Our developed metrics aim to bridge this gap in the literature, offering a comprehensive means of assessing question quality, which, we believe, will significantly contribute to the advancement of the Q&A generation process.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='4.2 Answer Evaluation\\n\\nEvaluating answers generated by LLMs is challenging due to their tendency of generating long, informative, and conversational answers. Traditional metrics are unsuitable to evaluate this type of answers (Kamalloo et al., 2023; Adlakha et al., 2023). Recent works have shown that LLMs themselves are judges that display high agreement with humans (Chiang and Lee, 2023; Dubois et al., 2023) and can be used directly or in conjunction with other techniques to evaluate answers to questions with or without context (Min et al., 2023). There is also promise in fine-tuning\\n\\n15\\n\\nQuestion\\n\\nCoverage Score ↑\\n\\nExplanation\\n\\nWhat are the potential impacts of climate change on dryland wheat production systems in the in- land Pacific Northwest?\\n\\n5', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='4.1 Question Evaluation\\n\\nDefining metrics to evaluate the quality of questions generated by large language models (LLMs) is a challenging task due to several factors. The inherent complexity of natural language and the subjective nature of what constitutes a \"good\" question make it difficult to establish a universally accepted set of metrics.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='LLMs as judges for open-ended scenarios (Zhu et al., 2023). In this work, we have used the AzureML Model Evaluation (Microsoft, 2023), with the following metrics to compare the generated answers with ground truth:\\n\\nCoherence: Comparison of coherence between ground truths and predictions given the context. The metric provides a score between one to five, where one means that the answer lacks coherence and five means the answer has perfect coherency. An example is provided in Table 6.\\n\\nRelevance: Relevance measures how well the answer addresses the main aspects of the question based on the context. The metric rates from 1 to 5, where 5 means the answer has perfect relevance. An example is provided in Table 7.\\n\\nGroundedness: The metric defines weather the answer follows logically from the information contained in the context or not and provides and integer score to determine how grounded the answer is. An example is provided in Table 8.\\n\\n17\\n\\nQuestion\\n\\nRelevance Score ↑\\n\\nAnswer', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='Listing 7 an example question and answer from the India dataset. The answers from KVK are brief and consist of at most one sentence. Using the Vikaspedia data, we enhanced the answers from KVK to make them more detailed and clear for the farmers.\\n\\n4 Metrics\\n\\nThe primary purpose of this section is to establish a comprehensive set of metrics designed to guide the quality assessment of the Q&A generation process, particularly for the fine-tuning and RAG methodologies. A fundamental principle in machine learning is that the quality of the model is strongly influenced by the quality of the dataset it is trained on. In this context, to train or fine-tune a model that effectively generates high-quality Q&A pairs, we first need a robust dataset. The inputs of our framework is the Q&A pairs along with their associated context, and the output a quantification the quality of these pairs and, finally, of the fine-tuned model. The quality here is determined on a set of parameters.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='Details: We assess the level of detail in both the generated questions and answers by counting the number of tokens (words) in each. This metric provides insight into the depth and specificity of the content generated by the Q&A system. By employing these metrics, we can effectively evaluate and refine the Q&A generation process, ensuring that the generated content is informative, relevant, diverse, and grounded in the source material. This will ultimately lead to a more useful and effective Q&A generation system for the target audience.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='In the development of our metrics, several key factors must be taken into account. Firstly, the subjectivity inherent in question quality presents a significant challenge. Since opinions vary on what constitutes a relevant, informative, or engaging question, it’s crucial to devise metrics that can objectively evaluate quality amidst this subjectivity. Secondly, the metrics must consider the dependency of a question’s relevance and usefulness on its context. A question that provides valuable insights in one context may be deemed irrelevant in another, underscoring the need for context-aware metrics.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='\\n\\nIn the following experiments, we employed three LLMs, namely GPT-3, GPT-3.5 and GPT-4, to evaluate the quality of generated Q&A pairs. The pairs were generated under different contexts: No context, Context, and External context as explained in the next subsection 5.1.1. We used several metrics (defined on Section 4.1) to assess their quality, including Relevance, Global Relevance, Coverage, Overlap, Diversity, Details, and Fluency. The LLMs assessed the Q&A pairs on various metrics, scoring each on a scale from 1 to 5. For certain metrics such as Overlap and Diversity, we incorporated intricate methods like Kullback-Leibler (KL) divergence and Word Mover’s Distance (WMD) to measure the semantic similarity between the source text and the questions generated. The primary objective of this experimental setup was to comprehend how different Q&A generation techniques can influence the quality of the resulting Q&A pairs. We aimed to discern how various metrics perceive and gauge the quality of these pairs and to highlight any significant disparities in their evaluations. This insight is crucial for refining the Q&A generation process, as it ensures that the resulting content is informative, pertinent, diverse, and fluent.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='To overcome these challenges, researchers often resort to a combination of automated and human evaluation methods. Automated methods, such as ROUGE Lin (2004) or BLEU Papineni et al. (2002) scores, can provide some insights into the quality of the generated questions, while human evaluation can help assess more subjective aspects like relevance, novelty, and fluency. Nevertheless, defining a comprehensive set of metrics for evaluating the quality of questions generated by LLMs remains an open and complex problem in the field of natural language processing. The proposed metrics are designed to provide a comprehensive view of the performance and effectiveness of a Q&A generation system, allowing for targeted improvements and refinements and to filter the questions with low quality. The metrics we have developed to evaluate the questions are as follows:\\n\\n13\\n\\nQuestion\\n\\nRelevance Score ↑\\n\\nExplanation', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='Coverage: To gauge whether the generated answers can be directly extracted from the provided context and how well the model avoids hallucinating, we use LLMs to rate the answerability of each Q&A pair on a scale of 1 to 5. The prompt was the following: “Your task is to rate from 1 to 5 if the answer can be extracted from the context and the question”. A higher score indicates that the answer can be more reliably extracted from the context, ensuring the model’s output is grounded in the available information. An example is provided in Table 4.', metadata={'source': 'example_data/2401.08406.pdf'})]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked documents:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "10 [(Document(page_content='Details: We assess the level of detail in both the generated questions and answers by counting the number of tokens (words) in each. This metric provides insight into the depth and specificity of the content generated by the Q&A system. By employing these metrics, we can effectively evaluate and refine the Q&A generation process, ensuring that the generated content is informative, relevant, diverse, and grounded in the source material. This will ultimately lead to a more useful and effective Q&A generation system for the target audience.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.13917649), (Document(page_content='4.2 Answer Evaluation\\n\\nEvaluating answers generated by LLMs is challenging due to their tendency of generating long, informative, and conversational answers. Traditional metrics are unsuitable to evaluate this type of answers (Kamalloo et al., 2023; Adlakha et al., 2023). Recent works have shown that LLMs themselves are judges that display high agreement with humans (Chiang and Lee, 2023; Dubois et al., 2023) and can be used directly or in conjunction with other techniques to evaluate answers to questions with or without context (Min et al., 2023). There is also promise in fine-tuning\\n\\n15\\n\\nQuestion\\n\\nCoverage Score ↑\\n\\nExplanation\\n\\nWhat are the potential impacts of climate change on dryland wheat production systems in the in- land Pacific Northwest?\\n\\n5', metadata={'source': 'example_data/2401.08406.pdf'}), 0.1034888), (Document(page_content='4.1 Question Evaluation\\n\\nDefining metrics to evaluate the quality of questions generated by large language models (LLMs) is a challenging task due to several factors. The inherent complexity of natural language and the subjective nature of what constitutes a \"good\" question make it difficult to establish a universally accepted set of metrics.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.06927996), (Document(page_content='LLMs as judges for open-ended scenarios (Zhu et al., 2023). In this work, we have used the AzureML Model Evaluation (Microsoft, 2023), with the following metrics to compare the generated answers with ground truth:\\n\\nCoherence: Comparison of coherence between ground truths and predictions given the context. The metric provides a score between one to five, where one means that the answer lacks coherence and five means the answer has perfect coherency. An example is provided in Table 6.\\n\\nRelevance: Relevance measures how well the answer addresses the main aspects of the question based on the context. The metric rates from 1 to 5, where 5 means the answer has perfect relevance. An example is provided in Table 7.\\n\\nGroundedness: The metric defines weather the answer follows logically from the information contained in the context or not and provides and integer score to determine how grounded the answer is. An example is provided in Table 8.\\n\\n17\\n\\nQuestion\\n\\nRelevance Score ↑\\n\\nAnswer', metadata={'source': 'example_data/2401.08406.pdf'}), 0.031911787), (Document(page_content='\\n\\nIn the existing literature, it’s common to find a wealth of metrics designed to evaluate the quality of answers. These metrics play an indispensable role in ensuring that the answers provided by models are accurate, relevant, and effectively address the questions posed. However, there is a notable gap when it comes to metrics specifically designed to assess the quality of the questions themselves. Recognizing this lack, we have developed our metrics with a focus on evaluating question quality. Given the pivotal role that questions play in driving meaningful conversations and generating useful answers, it is equally important to ensure their quality as it is for the answers. This quantitative evaluation of the Q&A sets will not only help in refining the generation process but also ensure the relevance and accuracy of the output, ultimately enhancing the overall efficacy of our model. Our developed metrics aim to bridge this gap in the literature, offering a comprehensive means of assessing question quality, which, we believe, will significantly contribute to the advancement of the Q&A generation process.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.028626742), (Document(page_content='Listing 7 an example question and answer from the India dataset. The answers from KVK are brief and consist of at most one sentence. Using the Vikaspedia data, we enhanced the answers from KVK to make them more detailed and clear for the farmers.\\n\\n4 Metrics\\n\\nThe primary purpose of this section is to establish a comprehensive set of metrics designed to guide the quality assessment of the Q&A generation process, particularly for the fine-tuning and RAG methodologies. A fundamental principle in machine learning is that the quality of the model is strongly influenced by the quality of the dataset it is trained on. In this context, to train or fine-tune a model that effectively generates high-quality Q&A pairs, we first need a robust dataset. The inputs of our framework is the Q&A pairs along with their associated context, and the output a quantification the quality of these pairs and, finally, of the fine-tuned model. The quality here is determined on a set of parameters.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.019994672), (Document(page_content='Coverage: To gauge whether the generated answers can be directly extracted from the provided context and how well the model avoids hallucinating, we use LLMs to rate the answerability of each Q&A pair on a scale of 1 to 5. The prompt was the following: “Your task is to rate from 1 to 5 if the answer can be extracted from the context and the question”. A higher score indicates that the answer can be more reliably extracted from the context, ensuring the model’s output is grounded in the available information. An example is provided in Table 4.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.012684247), (Document(page_content='In the development of our metrics, several key factors must be taken into account. Firstly, the subjectivity inherent in question quality presents a significant challenge. Since opinions vary on what constitutes a relevant, informative, or engaging question, it’s crucial to devise metrics that can objectively evaluate quality amidst this subjectivity. Secondly, the metrics must consider the dependency of a question’s relevance and usefulness on its context. A question that provides valuable insights in one context may be deemed irrelevant in another, underscoring the need for context-aware metrics.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.00089175184), (Document(page_content='To overcome these challenges, researchers often resort to a combination of automated and human evaluation methods. Automated methods, such as ROUGE Lin (2004) or BLEU Papineni et al. (2002) scores, can provide some insights into the quality of the generated questions, while human evaluation can help assess more subjective aspects like relevance, novelty, and fluency. Nevertheless, defining a comprehensive set of metrics for evaluating the quality of questions generated by LLMs remains an open and complex problem in the field of natural language processing. The proposed metrics are designed to provide a comprehensive view of the performance and effectiveness of a Q&A generation system, allowing for targeted improvements and refinements and to filter the questions with low quality. The metrics we have developed to evaluate the questions are as follows:\\n\\n13\\n\\nQuestion\\n\\nRelevance Score ↑\\n\\nExplanation', metadata={'source': 'example_data/2401.08406.pdf'}), 0.0005932096), (Document(page_content='\\n\\nIn the following experiments, we employed three LLMs, namely GPT-3, GPT-3.5 and GPT-4, to evaluate the quality of generated Q&A pairs. The pairs were generated under different contexts: No context, Context, and External context as explained in the next subsection 5.1.1. We used several metrics (defined on Section 4.1) to assess their quality, including Relevance, Global Relevance, Coverage, Overlap, Diversity, Details, and Fluency. The LLMs assessed the Q&A pairs on various metrics, scoring each on a scale from 1 to 5. For certain metrics such as Overlap and Diversity, we incorporated intricate methods like Kullback-Leibler (KL) divergence and Word Mover’s Distance (WMD) to measure the semantic similarity between the source text and the questions generated. The primary objective of this experimental setup was to comprehend how different Q&A generation techniques can influence the quality of the resulting Q&A pairs. We aimed to discern how various metrics perceive and gauge the quality of these pairs and to highlight any significant disparities in their evaluations. This insight is crucial for refining the Q&A generation process, as it ensures that the resulting content is informative, pertinent, diverse, and fluent.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.00045677982)]\n",
      "Reranked document:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Details: We assess the level of detail in both the generated questions and answers by counting the number of tokens (words) in each. This metric provides insight into the depth and specificity of the content generated by the Q&A system. By employing these metrics, we can effectively evaluate and refine the Q&A generation process, ensuring that the generated content is informative, relevant, diverse, and grounded in the source material. This will ultimately lead to a more useful and effective Q&A generation system for the target audience.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "metadata: {'source': 'example_data/2401.08406.pdf'}\n",
      "==================================================================================================== \n",
      "\n",
      "\n",
      "Example 2: Query-> How many pdf data were collected from the USA?\n",
      "....................................................................................................\n",
      "Retrieved document:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "10 [Document(page_content='3.1 USA\\n\\nWe collected documents, handbooks, and reports publicly available online from the United States Department of Agriculture (USDA), state agriculture and consumer services agencies, and partners from the Land-Grant Institutions National Program. Available documents contain federal regulatory and policy information surrounding crop and livestock management, information on diseases and best practices, quality assurance and export regulations, details on assistance programs, as well as insurance and pricing guidelines. Collected data totals more than 23k PDF files with over 50M tokens, representing 44 states in the USA. We downloaded and preprocessed these files, extracting the textual information that could be used as input to the Q&A generation pipeline. To benchmark and evaluate the models, we employed the documents related to the Washington state, which comprises 573 files with over 2M tokens. We present in Listing 5 an example of content within these documents.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='4 Experiments\\n\\n4.1 Datasets\\n\\nWe gather data for pre-training from two primary sources: (1) IIT-CDIP Test Collection 1.0 [56] and (2) DocBank [57]. IIT-CDIP Test Collection 1.0 encompasses a vast repository of over 5 million documents, comprising more than 16 million document pages. This dataset is derived from documents related to legal proceedings against the tobacco industry during the 1990s. DocBank consists of 500K documents, each featuring distinct layouts and a single page per document. The relevant statistics for the datasets utilized in the pre-training are detailed in Table 2. We obtain a collection of 16.7 million pages comprising a total of 3.8 billion tokens.', metadata={'source': 'example_data/2401.00908.pdf'}), Document(page_content='There are multiple tools available online that extract information from PDFs (PDF2Text, 2023; PyPDF, 2023). However, many of them lack the ability to retrieve content in a structured way. For example, pdf2text is an open-source Python library offering methods to iterate over PDF’s pages and recover the textual information. We provide in Listing 1 the output of pdf2text over the document from Figure 2. The library is able to recover the textual information, but markers representing the beginning of a section or subsection are lost within the retrieved data, hindering our ability to reason over the document structure. Captions of tables and figures are also lost in conversion but sometimes contain critical information for the understanding of the document.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='Considering this, we employed GROBID (GeneRation Of BIbliographic Data) (GRO, 2008–2023), a machine learning library specifically tailored for extracting and processing data from scientific literature in PDF format. The goal is to transform unstructured PDF data into structured data in the form of TEI (Text Encoding Initiative) format (Consortium, 2023), efficiently managing large volumes of files. The use of GROBID, trained on a vast corpus of scientific articles, enables the recognition of a wide array of document elements and extraction of associated bibliographic data. We illustrate its capabilities in Listing 2 with the output of GROBID for the document from Figure 2.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='Following data acquisition, the pipeline proceeds to extract information from the collected documents. This step is crucial as it involves parsing complex and unstructured PDF files to recover the underlying content and structure. This process, detailed further in Section 2.2, employs robust text extraction tools and machine learning algorithms to recover textual, tabular, and visual information, while also identifying the semantic structure of the documents and possible cross-relations in them.\\n\\nThe next component of the pipeline is question and answer generation. The objective here is to generate contextually grounded and high-quality questions that accurately reflect the content of the extracted text. The methodology employs a framework to control the structural composition of both inputs and outputs, thereby augmenting the overall efficacy of response generation from language models. This part of the process is detailed in Section 2.3.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='major soil erosion events in the region.\",\\n\\n], \"refs\": []...\\n\\n}\\n\\n}\\n\\n}\\n\\nListing 2: JSON file extracted from the PDF of Figure 2. We use GROBID (GRO, 2008–2023), which can extract the content and some of the structure (e.g., section and subsection organization) from the document.\\n\\n8\\n\\nOnce obtained, we combine the supporting context and section content, prompting the LLM to generate a set of questions based on them. We provide an example of the question generation prompt in Listing 4. The prompt includes a system preamble that guides the LLM into formulating assessment questions related to industrial topics based on the document content. Whereas the user portion provides few-shot examples of the types of expected questions, as well as the content and supporting context used for generation. With this setup, the LLM generates a set of 5 to 15 questions per section of the document.\\n\\nAnswer the following questions with a single Yes or No:', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='From the GROBID-generated TEI files, we extracted a subset of the sections of the TEI files comprising the document metadata (title, authors, abstract), sections, tables, figure references, bibliography, and the content itself. Crucially, this phase underscores the belief that the structure of the text is as important as its content. The final objective is to convert the TEI files into more manageable JSON files that preserve not only the content, but also the structure of the original\\n\\n5\\n\\nFigure 2: Example of a document from Washington state present in our dataset. The diverse layouts of PDF files, which often include textual and visual data, pose a significant challenge in terms of extracting not just the content, but also the underlying structure.\\n\\n6\\n\\nAdvances in Dryland Farming in the Inland Pacific Northwest Georgine Yorgey and Chad Kruger, editorsFor Sanford Eigenbrode, in recognition of his resolute\\n\\neffort', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='2.2 PDF Information Extraction\\n\\nIn our study, the extraction of information and text structure from the collected documents is critical to the quality of the subsequent steps. However, this is a challenging task as the primary purpose of PDFs is to accurately display a document across different systems, and not for easy information extraction. The underlying structure of a PDF file does not map onto the logical structure of a document, i.e., sections, subsection, and associated content. Additionally, with documents originating from various sources, we observe their layouts and formatting to be complex and lack standardization, often presenting a mixture of tables, images, sidebars, and page footers. We present in Figure 2 an example of a PDF file in our dataset.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='With this in mind, the main objective of this step of the pipeline is to address the complexities inherent in processing data derived from a range of formatted PDF documents. This is achieved by leveraging robust text extraction tools and machine learning algorithms that employ advanced natural language processing techniques. The focus is not only in recovering the content of each file, but also its structure. Among other things, we are interested in discovering what are the sections and subsections, parsing the information presented in tables and diagrams, identifying cross-references within the document, and linking images with their caption and description. By retrieving the organization of the document, we can easily group information, reason over numerical data present in tables, and provide more consistent snippets of the text to the Q&A generation step. It is also very important that all available information is extracted from the document, with well-formed sentences.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='We measure recall to understand how well RAG retrieves the original content used to generate a question in our dataset. The questions were generated based on the 573 documents of the Washington state benchmark dataset, and, for each question, we stored the passage of text used as basis for its creation. Following the RAG setup detailed in Section 2.4, our goal is to retrieve snippets that are contained in the original excerpt for a given question. In this context, recall refers to how many times we are able to retrieve the original excerpt given a question. We aim for higher recall as LLMs showed to be able to filter out unrelated pieces of information and highlight essential bits as long as the correct context is present during the answer generation.', metadata={'source': 'example_data/2401.08406.pdf'})]\n",
      "Reranked documents:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "10 [(Document(page_content='3.1 USA\\n\\nWe collected documents, handbooks, and reports publicly available online from the United States Department of Agriculture (USDA), state agriculture and consumer services agencies, and partners from the Land-Grant Institutions National Program. Available documents contain federal regulatory and policy information surrounding crop and livestock management, information on diseases and best practices, quality assurance and export regulations, details on assistance programs, as well as insurance and pricing guidelines. Collected data totals more than 23k PDF files with over 50M tokens, representing 44 states in the USA. We downloaded and preprocessed these files, extracting the textual information that could be used as input to the Q&A generation pipeline. To benchmark and evaluate the models, we employed the documents related to the Washington state, which comprises 573 files with over 2M tokens. We present in Listing 5 an example of content within these documents.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.48873308), (Document(page_content='2.2 PDF Information Extraction\\n\\nIn our study, the extraction of information and text structure from the collected documents is critical to the quality of the subsequent steps. However, this is a challenging task as the primary purpose of PDFs is to accurately display a document across different systems, and not for easy information extraction. The underlying structure of a PDF file does not map onto the logical structure of a document, i.e., sections, subsection, and associated content. Additionally, with documents originating from various sources, we observe their layouts and formatting to be complex and lack standardization, often presenting a mixture of tables, images, sidebars, and page footers. We present in Figure 2 an example of a PDF file in our dataset.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.00055886916), (Document(page_content='From the GROBID-generated TEI files, we extracted a subset of the sections of the TEI files comprising the document metadata (title, authors, abstract), sections, tables, figure references, bibliography, and the content itself. Crucially, this phase underscores the belief that the structure of the text is as important as its content. The final objective is to convert the TEI files into more manageable JSON files that preserve not only the content, but also the structure of the original\\n\\n5\\n\\nFigure 2: Example of a document from Washington state present in our dataset. The diverse layouts of PDF files, which often include textual and visual data, pose a significant challenge in terms of extracting not just the content, but also the underlying structure.\\n\\n6\\n\\nAdvances in Dryland Farming in the Inland Pacific Northwest Georgine Yorgey and Chad Kruger, editorsFor Sanford Eigenbrode, in recognition of his resolute\\n\\neffort', metadata={'source': 'example_data/2401.08406.pdf'}), 0.00037627973), (Document(page_content='4 Experiments\\n\\n4.1 Datasets\\n\\nWe gather data for pre-training from two primary sources: (1) IIT-CDIP Test Collection 1.0 [56] and (2) DocBank [57]. IIT-CDIP Test Collection 1.0 encompasses a vast repository of over 5 million documents, comprising more than 16 million document pages. This dataset is derived from documents related to legal proceedings against the tobacco industry during the 1990s. DocBank consists of 500K documents, each featuring distinct layouts and a single page per document. The relevant statistics for the datasets utilized in the pre-training are detailed in Table 2. We obtain a collection of 16.7 million pages comprising a total of 3.8 billion tokens.', metadata={'source': 'example_data/2401.00908.pdf'}), 0.00030878728), (Document(page_content='major soil erosion events in the region.\",\\n\\n], \"refs\": []...\\n\\n}\\n\\n}\\n\\n}\\n\\nListing 2: JSON file extracted from the PDF of Figure 2. We use GROBID (GRO, 2008–2023), which can extract the content and some of the structure (e.g., section and subsection organization) from the document.\\n\\n8\\n\\nOnce obtained, we combine the supporting context and section content, prompting the LLM to generate a set of questions based on them. We provide an example of the question generation prompt in Listing 4. The prompt includes a system preamble that guides the LLM into formulating assessment questions related to industrial topics based on the document content. Whereas the user portion provides few-shot examples of the types of expected questions, as well as the content and supporting context used for generation. With this setup, the LLM generates a set of 5 to 15 questions per section of the document.\\n\\nAnswer the following questions with a single Yes or No:', metadata={'source': 'example_data/2401.08406.pdf'}), 0.00020899842), (Document(page_content='Following data acquisition, the pipeline proceeds to extract information from the collected documents. This step is crucial as it involves parsing complex and unstructured PDF files to recover the underlying content and structure. This process, detailed further in Section 2.2, employs robust text extraction tools and machine learning algorithms to recover textual, tabular, and visual information, while also identifying the semantic structure of the documents and possible cross-relations in them.\\n\\nThe next component of the pipeline is question and answer generation. The objective here is to generate contextually grounded and high-quality questions that accurately reflect the content of the extracted text. The methodology employs a framework to control the structural composition of both inputs and outputs, thereby augmenting the overall efficacy of response generation from language models. This part of the process is detailed in Section 2.3.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.00015295723), (Document(page_content='With this in mind, the main objective of this step of the pipeline is to address the complexities inherent in processing data derived from a range of formatted PDF documents. This is achieved by leveraging robust text extraction tools and machine learning algorithms that employ advanced natural language processing techniques. The focus is not only in recovering the content of each file, but also its structure. Among other things, we are interested in discovering what are the sections and subsections, parsing the information presented in tables and diagrams, identifying cross-references within the document, and linking images with their caption and description. By retrieving the organization of the document, we can easily group information, reason over numerical data present in tables, and provide more consistent snippets of the text to the Q&A generation step. It is also very important that all available information is extracted from the document, with well-formed sentences.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.00011137184), (Document(page_content='Considering this, we employed GROBID (GeneRation Of BIbliographic Data) (GRO, 2008–2023), a machine learning library specifically tailored for extracting and processing data from scientific literature in PDF format. The goal is to transform unstructured PDF data into structured data in the form of TEI (Text Encoding Initiative) format (Consortium, 2023), efficiently managing large volumes of files. The use of GROBID, trained on a vast corpus of scientific articles, enables the recognition of a wide array of document elements and extraction of associated bibliographic data. We illustrate its capabilities in Listing 2 with the output of GROBID for the document from Figure 2.', metadata={'source': 'example_data/2401.08406.pdf'}), 9.9420766e-05), (Document(page_content='We measure recall to understand how well RAG retrieves the original content used to generate a question in our dataset. The questions were generated based on the 573 documents of the Washington state benchmark dataset, and, for each question, we stored the passage of text used as basis for its creation. Following the RAG setup detailed in Section 2.4, our goal is to retrieve snippets that are contained in the original excerpt for a given question. In this context, recall refers to how many times we are able to retrieve the original excerpt given a question. We aim for higher recall as LLMs showed to be able to filter out unrelated pieces of information and highlight essential bits as long as the correct context is present during the answer generation.', metadata={'source': 'example_data/2401.08406.pdf'}), 9.8382785e-05), (Document(page_content='There are multiple tools available online that extract information from PDFs (PDF2Text, 2023; PyPDF, 2023). However, many of them lack the ability to retrieve content in a structured way. For example, pdf2text is an open-source Python library offering methods to iterate over PDF’s pages and recover the textual information. We provide in Listing 1 the output of pdf2text over the document from Figure 2. The library is able to recover the textual information, but markers representing the beginning of a section or subsection are lost within the retrieved data, hindering our ability to reason over the document structure. Captions of tables and figures are also lost in conversion but sometimes contain critical information for the understanding of the document.', metadata={'source': 'example_data/2401.08406.pdf'}), 9.151485e-05)]\n",
      "Reranked document:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "3.1 USA\n",
      "\n",
      "We collected documents, handbooks, and reports publicly available online from the United States Department of Agriculture (USDA), state agriculture and consumer services agencies, and partners from the Land-Grant Institutions National Program. Available documents contain federal regulatory and policy information surrounding crop and livestock management, information on diseases and best practices, quality assurance and export regulations, details on assistance programs, as well as insurance and pricing guidelines. Collected data totals more than 23k PDF files with over 50M tokens, representing 44 states in the USA. We downloaded and preprocessed these files, extracting the textual information that could be used as input to the Q&A generation pipeline. To benchmark and evaluate the models, we employed the documents related to the Washington state, which comprises 573 files with over 2M tokens. We present in Listing 5 an example of content within these documents.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "metadata: {'source': 'example_data/2401.08406.pdf'}\n",
      "==================================================================================================== \n",
      "\n",
      "\n",
      "Example 3: Query-> What is the DocLLM architecture ?\n",
      "....................................................................................................\n",
      "Retrieved document:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "10 [Document(page_content='4.2 Model Setup and Training Details\\n\\nTable 4 provides key settings and hyperparameters for two variants of DocLLM: DocLLM-1B, which is based on the Falcon-1B architecture [5], and DocLLM-7B, which is based on the Llama2-7B architecture [4]3. DocLLM-1B is composed of 24 layers, each with 16 attention heads and a hidden size of 1,536. DocLLM-7B comprises 36 layers, 32 heads, and a hidden size of 4,096. Using pre-trained weights as the backbone for the text modality, we extend the Falcon-1B and Llama2-7B models by adding the disentangled attention and block infilling objective as described in Section 3. For DocLLM-1B, we use a pre-training learning rate of 2 × 10−4 with 1,000 warmup steps, employing a cosine scheduler, and Adam optimizer [58] with β1 = 0.9, β2 = 0.96 and a weight decay of 0.1. For instruction tuning we use a learning\\n\\n3Since Llama2 does not come with pre-trained weights at 1B parameters, we use the Falcon-1B architecture for the smaller\\n\\nversion of DocLLM.\\n\\n7', metadata={'source': 'example_data/2401.00908.pdf'}), Document(page_content='The maximum sequence length, or context length, is consistently set to 1,024 for both versions during the entire training process. The DocLLM-7B models are trained with 16-bit mixed precision on 8 24GB A10g GPUs using fully sharded data parallelism, implemented with the accelerate library.4 The DocLLM-1B model, on the other hand, is trained on a single 24GB A10g GPU.\\n\\n4.3 Downstream Evaluation\\n\\nExperimental settings. We investigate two experimental settings:\\n\\nSame Datasets, Different Splits (SDDS): Following previous work in VRDU [34, 59, 33, 12, 31, 32], we first evaluate DocLLM on the unseen test split (or dev split when test split is unavailable) of each of the 16 datasets composing the instruction-tuning data. The motivation behind this very typical setting is to check how DocLLM performs when tasks and domains supposedly stay the same from train to test.', metadata={'source': 'example_data/2401.00908.pdf'}), Document(page_content='We adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets. These tasks encompass key information extraction, natural language inference, visual question-answering and document classification. Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents. We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 15% to 61% for the Llama2-7B model in four out of five previously unseen datasets.\\n\\nFig. 1 summarizes the framework. Our contributions include:\\n\\n1. A light-weight extension to LLMs designed for understanding visual documents.\\n\\n2. A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.', metadata={'source': 'example_data/2401.00908.pdf'}), Document(page_content='6 Discussion and Findings\\n\\nIn addition to its immediate utility in visually rich document understanding tasks, we posit that DocLLM offers an opportunity to change the landscape of generative pre-training by enabling language models to go beyond next token prediction in plain text settings. By accommodating complex layout structures, DocLLM allows for e-books, e-publications, and other documents with rich layouts to be incorporated into the pre-training corpus without requiring extensive preprocessing. The spatial-aware reading approach enables the model to perceive the document as inherently structured knowledge.', metadata={'source': 'example_data/2401.00908.pdf'}), Document(page_content='\\n\\nSame Tasks, Different Datasets (STDD): Following [40, 41, 60, 61], we also evaluate DocLLM on held-out datasets. More precisely, we instruction-tune the pre-trained checkpoint of DocLLM on prompts from 11 of the 16 datasets considered in SDDS, then evaluate DocLLM on the test split of the remaining three datasets. The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged but domains and layouts differ from train to test. We believe examining this setting in the DocAI field is relevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS, while document characteristics tend to change more often in production. We specifically isolate DocVQA, KLC, and BizDocs for STDD evaluation in order to (1) exclude at least one dataset per task from SFT when possible, (2) leave enough datapoints per task in the training split of the instruction-tuning data, (3) avoid data leakage (certain datasets were obtained from the same sources), and (4) benchmark models on popular yet challenging datasets when possible. Due to the high cost of instruction-tuning, we were not able to run additional experiments with different held-out datasets.', metadata={'source': 'example_data/2401.00908.pdf'}), Document(page_content='Results. In the SDDS setting, as shown in the Table 5, we observe that DocLLM-7B excels in 12 out of 16 datasets, inclusively compared to ZS results of GPT4 and Llama2, and SDDS results of mPLUG-DocOwl and UReader. Among equivalent models (excluding GPT4), our model outperforms in 14 out of 16 datasets. Specifically, DocLLM demonstrates superior performance in layout-intensive tasks such as KIE and CLS. In VQA and NLI, its performance surpasses that of most multimodal language models, although it underperforms compared to GPT-4. GPT-4 outperforms DocLLM in VQA, possibly due to the higher complexity of reasoning and abstraction involved in VQA datasets compared to tasks like KIE or CLS. DocLLM-1B demonstrates performance close to that of our larger model, suggesting that the smaller model can derive significant benefits from the architecture of DocLLM.', metadata={'source': 'example_data/2401.00908.pdf'}), Document(page_content='LIF(θ) = −\\n\\nM (cid:88)\\n\\nNm(cid:88)\\n\\nlog pθ(zm,j|˜x, z<m, zm,<j)\\n\\n(6)\\n\\nm=1\\n\\nj=1\\n\\n3.4\\n\\nInstruction Tuning\\n\\nFollowing recent work in the field of VRDU [12, 31, 32] and prior work in NLP [40, 41], we instruction-tune DocLLM on a variety of instructions derived from DocAI datasets using various templates. Due to the high cost and time intensity of manual data collection, we leave the construction of a VRDU instruction-tuning dataset with crowdsourced instructions and preferences to future work. We employ a total of 16 datasets with their corresponding OCRs, spanning four DocAI tasks: visual question answering (VQA), natural language inference (NLI), key information extraction (KIE), and document classification (CLS).', metadata={'source': 'example_data/2401.00908.pdf'}), Document(page_content='-\\n\\nDocOwl\\n\\n∼1T (T) ZS\\n\\n7B (T) ZS\\n\\n∼7B (T+V) SDDS\\n\\nUReader ∼7B (T+V) SDDS\\n\\nDocLLM\\n\\n-\\n\\n1B DocLLM\\n\\n-\\n\\n7B\\n\\n1B (T+L) SDDS\\n\\n7B (T+L) SDDS\\n\\nVQA\\n\\nDocVQA WTQ (Accuracy) VisualMRC (CIDEr) DUDE BizDocs\\n\\n82.8 65.4 255.1 54.6 76.4\\n\\n47.4 25.0 115.5 38.1 48.8\\n\\n62.2 26.9 188.8 \\n\\n-\\n\\n -\\n\\n65.4 29.4 221.7 \\n\\n-\\n\\n -\\n\\n61.4 21.9 245.0 42.6 84.5\\n\\n69.5 27.1 264.1 47.2 86.7\\n\\nNLI\\n\\nTabFact\\n\\n77.1\\n\\n48.2\\n\\n60.2\\n\\n67.6\\n\\n58.0\\n\\n66.4\\n\\nKIE\\n\\nKLC CORD FUNSD DeepForm PWC SROIE VRDU a.-b. BizDocs\\n\\n45.9 58.3 37.0 42.1 18.3 90.6 43.7 66.1\\n\\n27.8 13.8 17.8 20.5 6.8 56.4 18.7 10.8\\n\\n30.3 \\n\\n-\\n\\n-\\n\\n 42.6 \\n\\n-\\n\\n-\\n\\n-\\n\\n -\\n\\n32.8 \\n\\n-\\n\\n-\\n\\n 49.5 \\n\\n-\\n\\n-\\n\\n-\\n\\n -\\n\\n58.9 66.9 48.2 71.3 25.7 91.0 87.6 95.4\\n\\n60.3 67.4 51.8 75.7 29.06 91.9 88.8 95.9\\n\\nCLS\\n\\nRVL\\n\\n-\\n\\nCDIP BizDocs\\n\\n68.2 84.9\\n\\n32.8 40.9\\n\\n-\\n\\n-\\n\\n90.9 98.3\\n\\n91.8 99.4\\n\\nAs motivated in section 2, we do not consider DocAI models that require task-specific fine-tuning [33, 59, 34] and/or dataset specific prompts [12], and instead focus on LLMs with out-of-the-box instruction following capability.', metadata={'source': 'example_data/2401.00908.pdf'}), Document(page_content='\\n\\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure. It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network. Standard language models are typically unimodal, accepting only a sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained using OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout, especially for visually rich documents [10]. Consequently, we treat the spatial information about the text tokens as a distinct modality. In particular, we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner, as explained in the following section. Furthermore, instead of the traditional left-to-right next token prediction during self-supervised training, we employ a text infilling objective that better leverages contextual information.', metadata={'source': 'example_data/2401.00908.pdf'}), Document(page_content='In the STDD setting, our model demonstrates superior performance compared to Llama2 across four out of five datasets, and achieves the best score overall for two of them (KIE task again). DocLLM also outperforms mPLUG-DocOwl on DocVQA and both mPLUG-DocOwl and UReader on KLC, despite both baselines having been instruction-tuned on these datasets. However, it is important to note that classification accuracy is notably lower in our model. This discrepancy may stem from the fact that our model has been trained using only one classification dataset, limiting its ability to generalize effectively to new datasets.\\n\\n5 Ablation Studies\\n\\nWe conduct ablation studies to validate the three contributions of DocLLM: (1) disentangled spatial features, (2) the block infilling pre-training objective, and (3) the masking strategy used for decoding.', metadata={'source': 'example_data/2401.00908.pdf'})]\n",
      "Reranked documents:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "10 [(Document(page_content='\\n\\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure. It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network. Standard language models are typically unimodal, accepting only a sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained using OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout, especially for visually rich documents [10]. Consequently, we treat the spatial information about the text tokens as a distinct modality. In particular, we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner, as explained in the following section. Furthermore, instead of the traditional left-to-right next token prediction during self-supervised training, we employ a text infilling objective that better leverages contextual information.', metadata={'source': 'example_data/2401.00908.pdf'}), 0.9895678), (Document(page_content='4.2 Model Setup and Training Details\\n\\nTable 4 provides key settings and hyperparameters for two variants of DocLLM: DocLLM-1B, which is based on the Falcon-1B architecture [5], and DocLLM-7B, which is based on the Llama2-7B architecture [4]3. DocLLM-1B is composed of 24 layers, each with 16 attention heads and a hidden size of 1,536. DocLLM-7B comprises 36 layers, 32 heads, and a hidden size of 4,096. Using pre-trained weights as the backbone for the text modality, we extend the Falcon-1B and Llama2-7B models by adding the disentangled attention and block infilling objective as described in Section 3. For DocLLM-1B, we use a pre-training learning rate of 2 × 10−4 with 1,000 warmup steps, employing a cosine scheduler, and Adam optimizer [58] with β1 = 0.9, β2 = 0.96 and a weight decay of 0.1. For instruction tuning we use a learning\\n\\n3Since Llama2 does not come with pre-trained weights at 1B parameters, we use the Falcon-1B architecture for the smaller\\n\\nversion of DocLLM.\\n\\n7', metadata={'source': 'example_data/2401.00908.pdf'}), 0.525848), (Document(page_content='6 Discussion and Findings\\n\\nIn addition to its immediate utility in visually rich document understanding tasks, we posit that DocLLM offers an opportunity to change the landscape of generative pre-training by enabling language models to go beyond next token prediction in plain text settings. By accommodating complex layout structures, DocLLM allows for e-books, e-publications, and other documents with rich layouts to be incorporated into the pre-training corpus without requiring extensive preprocessing. The spatial-aware reading approach enables the model to perceive the document as inherently structured knowledge.', metadata={'source': 'example_data/2401.00908.pdf'}), 0.4741518), (Document(page_content='The maximum sequence length, or context length, is consistently set to 1,024 for both versions during the entire training process. The DocLLM-7B models are trained with 16-bit mixed precision on 8 24GB A10g GPUs using fully sharded data parallelism, implemented with the accelerate library.4 The DocLLM-1B model, on the other hand, is trained on a single 24GB A10g GPU.\\n\\n4.3 Downstream Evaluation\\n\\nExperimental settings. We investigate two experimental settings:\\n\\nSame Datasets, Different Splits (SDDS): Following previous work in VRDU [34, 59, 33, 12, 31, 32], we first evaluate DocLLM on the unseen test split (or dev split when test split is unavailable) of each of the 16 datasets composing the instruction-tuning data. The motivation behind this very typical setting is to check how DocLLM performs when tasks and domains supposedly stay the same from train to test.', metadata={'source': 'example_data/2401.00908.pdf'}), 0.019657673), (Document(page_content='-\\n\\nDocOwl\\n\\n∼1T (T) ZS\\n\\n7B (T) ZS\\n\\n∼7B (T+V) SDDS\\n\\nUReader ∼7B (T+V) SDDS\\n\\nDocLLM\\n\\n-\\n\\n1B DocLLM\\n\\n-\\n\\n7B\\n\\n1B (T+L) SDDS\\n\\n7B (T+L) SDDS\\n\\nVQA\\n\\nDocVQA WTQ (Accuracy) VisualMRC (CIDEr) DUDE BizDocs\\n\\n82.8 65.4 255.1 54.6 76.4\\n\\n47.4 25.0 115.5 38.1 48.8\\n\\n62.2 26.9 188.8 \\n\\n-\\n\\n -\\n\\n65.4 29.4 221.7 \\n\\n-\\n\\n -\\n\\n61.4 21.9 245.0 42.6 84.5\\n\\n69.5 27.1 264.1 47.2 86.7\\n\\nNLI\\n\\nTabFact\\n\\n77.1\\n\\n48.2\\n\\n60.2\\n\\n67.6\\n\\n58.0\\n\\n66.4\\n\\nKIE\\n\\nKLC CORD FUNSD DeepForm PWC SROIE VRDU a.-b. BizDocs\\n\\n45.9 58.3 37.0 42.1 18.3 90.6 43.7 66.1\\n\\n27.8 13.8 17.8 20.5 6.8 56.4 18.7 10.8\\n\\n30.3 \\n\\n-\\n\\n-\\n\\n 42.6 \\n\\n-\\n\\n-\\n\\n-\\n\\n -\\n\\n32.8 \\n\\n-\\n\\n-\\n\\n 49.5 \\n\\n-\\n\\n-\\n\\n-\\n\\n -\\n\\n58.9 66.9 48.2 71.3 25.7 91.0 87.6 95.4\\n\\n60.3 67.4 51.8 75.7 29.06 91.9 88.8 95.9\\n\\nCLS\\n\\nRVL\\n\\n-\\n\\nCDIP BizDocs\\n\\n68.2 84.9\\n\\n32.8 40.9\\n\\n-\\n\\n-\\n\\n90.9 98.3\\n\\n91.8 99.4\\n\\nAs motivated in section 2, we do not consider DocAI models that require task-specific fine-tuning [33, 59, 34] and/or dataset specific prompts [12], and instead focus on LLMs with out-of-the-box instruction following capability.', metadata={'source': 'example_data/2401.00908.pdf'}), 0.010306237), (Document(page_content='We adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets. These tasks encompass key information extraction, natural language inference, visual question-answering and document classification. Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents. We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 15% to 61% for the Llama2-7B model in four out of five previously unseen datasets.\\n\\nFig. 1 summarizes the framework. Our contributions include:\\n\\n1. A light-weight extension to LLMs designed for understanding visual documents.\\n\\n2. A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.', metadata={'source': 'example_data/2401.00908.pdf'}), 0.002986798), (Document(page_content='Results. In the SDDS setting, as shown in the Table 5, we observe that DocLLM-7B excels in 12 out of 16 datasets, inclusively compared to ZS results of GPT4 and Llama2, and SDDS results of mPLUG-DocOwl and UReader. Among equivalent models (excluding GPT4), our model outperforms in 14 out of 16 datasets. Specifically, DocLLM demonstrates superior performance in layout-intensive tasks such as KIE and CLS. In VQA and NLI, its performance surpasses that of most multimodal language models, although it underperforms compared to GPT-4. GPT-4 outperforms DocLLM in VQA, possibly due to the higher complexity of reasoning and abstraction involved in VQA datasets compared to tasks like KIE or CLS. DocLLM-1B demonstrates performance close to that of our larger model, suggesting that the smaller model can derive significant benefits from the architecture of DocLLM.', metadata={'source': 'example_data/2401.00908.pdf'}), 0.0017841017), (Document(page_content='In the STDD setting, our model demonstrates superior performance compared to Llama2 across four out of five datasets, and achieves the best score overall for two of them (KIE task again). DocLLM also outperforms mPLUG-DocOwl on DocVQA and both mPLUG-DocOwl and UReader on KLC, despite both baselines having been instruction-tuned on these datasets. However, it is important to note that classification accuracy is notably lower in our model. This discrepancy may stem from the fact that our model has been trained using only one classification dataset, limiting its ability to generalize effectively to new datasets.\\n\\n5 Ablation Studies\\n\\nWe conduct ablation studies to validate the three contributions of DocLLM: (1) disentangled spatial features, (2) the block infilling pre-training objective, and (3) the masking strategy used for decoding.', metadata={'source': 'example_data/2401.00908.pdf'}), 0.0010601969), (Document(page_content='LIF(θ) = −\\n\\nM (cid:88)\\n\\nNm(cid:88)\\n\\nlog pθ(zm,j|˜x, z<m, zm,<j)\\n\\n(6)\\n\\nm=1\\n\\nj=1\\n\\n3.4\\n\\nInstruction Tuning\\n\\nFollowing recent work in the field of VRDU [12, 31, 32] and prior work in NLP [40, 41], we instruction-tune DocLLM on a variety of instructions derived from DocAI datasets using various templates. Due to the high cost and time intensity of manual data collection, we leave the construction of a VRDU instruction-tuning dataset with crowdsourced instructions and preferences to future work. We employ a total of 16 datasets with their corresponding OCRs, spanning four DocAI tasks: visual question answering (VQA), natural language inference (NLI), key information extraction (KIE), and document classification (CLS).', metadata={'source': 'example_data/2401.00908.pdf'}), 0.0006377195), (Document(page_content='\\n\\nSame Tasks, Different Datasets (STDD): Following [40, 41, 60, 61], we also evaluate DocLLM on held-out datasets. More precisely, we instruction-tune the pre-trained checkpoint of DocLLM on prompts from 11 of the 16 datasets considered in SDDS, then evaluate DocLLM on the test split of the remaining three datasets. The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged but domains and layouts differ from train to test. We believe examining this setting in the DocAI field is relevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS, while document characteristics tend to change more often in production. We specifically isolate DocVQA, KLC, and BizDocs for STDD evaluation in order to (1) exclude at least one dataset per task from SFT when possible, (2) leave enough datapoints per task in the training split of the instruction-tuning data, (3) avoid data leakage (certain datasets were obtained from the same sources), and (4) benchmark models on popular yet challenging datasets when possible. Due to the high cost of instruction-tuning, we were not able to run additional experiments with different held-out datasets.', metadata={'source': 'example_data/2401.00908.pdf'}), 0.00024628872)]\n",
      "Reranked document:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "DocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure. It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network. Standard language models are typically unimodal, accepting only a sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained using OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout, especially for visually rich documents [10]. Consequently, we treat the spatial information about the text tokens as a distinct modality. In particular, we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner, as explained in the following section. Furthermore, instead of the traditional left-to-right next token prediction during self-supervised training, we employ a text infilling objective that better leverages contextual information.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "metadata: {'source': 'example_data/2401.00908.pdf'}\n",
      "==================================================================================================== \n",
      "\n",
      "\n",
      "Example 4: Query-> Which contries were used to collect dataset?\n",
      "....................................................................................................\n",
      "Retrieved document:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "10 [Document(page_content='3 Dataset Overview\\n\\nThis study evaluates fine-tuned and RAG-enhanced language models using context-related questions and answers originated datasets from three major crop producer countries: USA, Brazil and India. In our case, we are using agriculture as the industrial setting as an example. Available data varied considerably in format and content, ranging from regulatory documents, scientific reports, agronomic exams, to knowledge databases. In this section, we present each dataset in more detail.\\n\\n3.1 USA', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='4 Experiments\\n\\n4.1 Datasets\\n\\nWe gather data for pre-training from two primary sources: (1) IIT-CDIP Test Collection 1.0 [56] and (2) DocBank [57]. IIT-CDIP Test Collection 1.0 encompasses a vast repository of over 5 million documents, comprising more than 16 million document pages. This dataset is derived from documents related to legal proceedings against the tobacco industry during the 1990s. DocBank consists of 500K documents, each featuring distinct layouts and a single page per document. The relevant statistics for the datasets utilized in the pre-training are detailed in Table 2. We obtain a collection of 16.7 million pages comprising a total of 3.8 billion tokens.', metadata={'source': 'example_data/2401.00908.pdf'}), Document(page_content='Listing 7 an example question and answer from the India dataset. The answers from KVK are brief and consist of at most one sentence. Using the Vikaspedia data, we enhanced the answers from KVK to make them more detailed and clear for the farmers.\\n\\n4 Metrics\\n\\nThe primary purpose of this section is to establish a comprehensive set of metrics designed to guide the quality assessment of the Q&A generation process, particularly for the fine-tuning and RAG methodologies. A fundamental principle in machine learning is that the quality of the model is strongly influenced by the quality of the dataset it is trained on. In this context, to train or fine-tune a model that effectively generates high-quality Q&A pairs, we first need a robust dataset. The inputs of our framework is the Q&A pairs along with their associated context, and the output a quantification the quality of these pairs and, finally, of the fine-tuned model. The quality here is determined on a set of parameters.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='2 Methodology\\n\\nThe methodology proposed in this paper revolves around a pipeline designed to generate and evaluate question-answer pairs for building domain-specific copilots. The proposed pipeline is shown in Figure 1.\\n\\nThe pipeline begins with data acquisition, described in Section 2.1. The initial focus is on gathering a diverse and curated dataset pertinent to the industry domain. This includes sourcing data from various high-quality repositories such as government agencies, scientific knowledge databases, and proprietary data, if needed. The details of potential data sources and the types of documents selected are exemplified and further elaborated in Section 3.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='Key Information Extraction. We gather Kleister Charity (KLC) [48], CORD [49], FUNSD [50], DeepForm [51], PWC [52], SROIE [53], VRDU ad-buy [54] (with random train-test splitting), and BizDocs to build the KIE instruction-tuning data, where we leverage three instruction templates: extraction, internal classification, and MCQ, as shown in 1. For the\\n\\n2BizDocs is a collection of business entity filings that is due to be released publicly.\\n\\n6\\n\\nTable 2: Pre-training dataset statistics. No. of Docs No. of Pages No. of Total Tokens\\n\\nCDIP DocBank Total\\n\\n5,092,636 499,609 5,592,245\\n\\n16,293,353 499,609 16,792,962\\n\\n3,637,551,478 228,362,274 3,865,913,752\\n\\nTable 3: Instruction-tuning dataset statistics. Tasks No. of Training No. of Testing\\n\\nVQA NLI KIE CLS Total\\n\\n145,090 104,360 236,806 149,627 635,883\\n\\n24,347 12,720 38,039 21,813 96,919', metadata={'source': 'example_data/2401.00908.pdf'}), Document(page_content='The remainder of this work is organized as follows. Section 2 presents the methodology in detail, including the data acquisition process, information extraction procedure, question and answer generation, and fine-tuning of the model. We then describe in Section 3 the dataset used in the study, which includes data from the USA, Brazil, and India. In Section 4, we outline the metrics used to evaluate the effectiveness of the proposed methodology, focusing on both question and answer evaluation. Section 5 presents a comprehensive evaluation of various models and their performance in generating question-answer pairs within the context of agricultural data, using these for RAG on GPT-4, Vicuna (Zheng et al., 2023) and Llama2 13B (Touvron et al., 2023a,b) and fine-tuning of GPT-4 and Llama2 13B. Finally, we conclude with a summary of the main findings and possible directions for future research in Section 6.\\n\\n2 Methodology', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='Listing 6: Example of an adapted question from the Embrapa dataset, focusing on the symptoms and damages caused by the gall nematode in citrus plants.\\n\\n3.3\\n\\nIndia', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='\\n\\nSame Tasks, Different Datasets (STDD): Following [40, 41, 60, 61], we also evaluate DocLLM on held-out datasets. More precisely, we instruction-tune the pre-trained checkpoint of DocLLM on prompts from 11 of the 16 datasets considered in SDDS, then evaluate DocLLM on the test split of the remaining three datasets. The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged but domains and layouts differ from train to test. We believe examining this setting in the DocAI field is relevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS, while document characteristics tend to change more often in production. We specifically isolate DocVQA, KLC, and BizDocs for STDD evaluation in order to (1) exclude at least one dataset per task from SFT when possible, (2) leave enough datapoints per task in the training split of the instruction-tuning data, (3) avoid data leakage (certain datasets were obtained from the same sources), and (4) benchmark models on popular yet challenging datasets when possible. Due to the high cost of instruction-tuning, we were not able to run additional experiments with different held-out datasets.', metadata={'source': 'example_data/2401.00908.pdf'}), Document(page_content='We have introduced the datasets used to conduct instruction tuning on Section 3.4. These datasets encompass four common DocAI tasks: VQA, NLI, KIE, and CLS. Note that when a prompt includes a list of possible answers, we create multiple copies of the prompt with one possible answer assigned to each. We only perform this “flattening” operation in the training split of the dataset. Detailed statistics for these tasks are presented in Table 3.\\n\\n4.2 Model Setup and Training Details', metadata={'source': 'example_data/2401.00908.pdf'}), Document(page_content='2.1 Data Acquisition\\n\\nThe initial focus of the pipeline is to gather an assorted and well-curated dataset that captures information of interest to an industry. This enables the generation of questions and answers, forming the foundation for refining the models to produce more precise and pertinent responses. For this step, we seek data sources that contain high-quality, authoritative information on the topic of interest. For example, in agriculture, this includes agricultural and environmental government agencies, scientific knowledge repositories, and agronomist exams databases. It is also important that the information extracted to be aligned with the grounding that will be provided to the model. For instance, in the case of agricultural data, we sourced guidelines and procedures that were geography-specific, i.e. with a shared location among documents. We provide further details on the sources and type of documents selected for this step in Section 3.', metadata={'source': 'example_data/2401.08406.pdf'})]\n",
      "Reranked documents:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "10 [(Document(page_content='3 Dataset Overview\\n\\nThis study evaluates fine-tuned and RAG-enhanced language models using context-related questions and answers originated datasets from three major crop producer countries: USA, Brazil and India. In our case, we are using agriculture as the industrial setting as an example. Available data varied considerably in format and content, ranging from regulatory documents, scientific reports, agronomic exams, to knowledge databases. In this section, we present each dataset in more detail.\\n\\n3.1 USA', metadata={'source': 'example_data/2401.08406.pdf'}), 0.3778102), (Document(page_content='4 Experiments\\n\\n4.1 Datasets\\n\\nWe gather data for pre-training from two primary sources: (1) IIT-CDIP Test Collection 1.0 [56] and (2) DocBank [57]. IIT-CDIP Test Collection 1.0 encompasses a vast repository of over 5 million documents, comprising more than 16 million document pages. This dataset is derived from documents related to legal proceedings against the tobacco industry during the 1990s. DocBank consists of 500K documents, each featuring distinct layouts and a single page per document. The relevant statistics for the datasets utilized in the pre-training are detailed in Table 2. We obtain a collection of 16.7 million pages comprising a total of 3.8 billion tokens.', metadata={'source': 'example_data/2401.00908.pdf'}), 0.0024847763), (Document(page_content='2 Methodology\\n\\nThe methodology proposed in this paper revolves around a pipeline designed to generate and evaluate question-answer pairs for building domain-specific copilots. The proposed pipeline is shown in Figure 1.\\n\\nThe pipeline begins with data acquisition, described in Section 2.1. The initial focus is on gathering a diverse and curated dataset pertinent to the industry domain. This includes sourcing data from various high-quality repositories such as government agencies, scientific knowledge databases, and proprietary data, if needed. The details of potential data sources and the types of documents selected are exemplified and further elaborated in Section 3.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.0016932978), (Document(page_content='2.1 Data Acquisition\\n\\nThe initial focus of the pipeline is to gather an assorted and well-curated dataset that captures information of interest to an industry. This enables the generation of questions and answers, forming the foundation for refining the models to produce more precise and pertinent responses. For this step, we seek data sources that contain high-quality, authoritative information on the topic of interest. For example, in agriculture, this includes agricultural and environmental government agencies, scientific knowledge repositories, and agronomist exams databases. It is also important that the information extracted to be aligned with the grounding that will be provided to the model. For instance, in the case of agricultural data, we sourced guidelines and procedures that were geography-specific, i.e. with a shared location among documents. We provide further details on the sources and type of documents selected for this step in Section 3.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.0010285837), (Document(page_content='Listing 7 an example question and answer from the India dataset. The answers from KVK are brief and consist of at most one sentence. Using the Vikaspedia data, we enhanced the answers from KVK to make them more detailed and clear for the farmers.\\n\\n4 Metrics\\n\\nThe primary purpose of this section is to establish a comprehensive set of metrics designed to guide the quality assessment of the Q&A generation process, particularly for the fine-tuning and RAG methodologies. A fundamental principle in machine learning is that the quality of the model is strongly influenced by the quality of the dataset it is trained on. In this context, to train or fine-tune a model that effectively generates high-quality Q&A pairs, we first need a robust dataset. The inputs of our framework is the Q&A pairs along with their associated context, and the output a quantification the quality of these pairs and, finally, of the fine-tuned model. The quality here is determined on a set of parameters.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.0009398422), (Document(page_content='The remainder of this work is organized as follows. Section 2 presents the methodology in detail, including the data acquisition process, information extraction procedure, question and answer generation, and fine-tuning of the model. We then describe in Section 3 the dataset used in the study, which includes data from the USA, Brazil, and India. In Section 4, we outline the metrics used to evaluate the effectiveness of the proposed methodology, focusing on both question and answer evaluation. Section 5 presents a comprehensive evaluation of various models and their performance in generating question-answer pairs within the context of agricultural data, using these for RAG on GPT-4, Vicuna (Zheng et al., 2023) and Llama2 13B (Touvron et al., 2023a,b) and fine-tuning of GPT-4 and Llama2 13B. Finally, we conclude with a summary of the main findings and possible directions for future research in Section 6.\\n\\n2 Methodology', metadata={'source': 'example_data/2401.08406.pdf'}), 0.00046503596), (Document(page_content='Listing 6: Example of an adapted question from the Embrapa dataset, focusing on the symptoms and damages caused by the gall nematode in citrus plants.\\n\\n3.3\\n\\nIndia', metadata={'source': 'example_data/2401.08406.pdf'}), 0.00044709313), (Document(page_content='Key Information Extraction. We gather Kleister Charity (KLC) [48], CORD [49], FUNSD [50], DeepForm [51], PWC [52], SROIE [53], VRDU ad-buy [54] (with random train-test splitting), and BizDocs to build the KIE instruction-tuning data, where we leverage three instruction templates: extraction, internal classification, and MCQ, as shown in 1. For the\\n\\n2BizDocs is a collection of business entity filings that is due to be released publicly.\\n\\n6\\n\\nTable 2: Pre-training dataset statistics. No. of Docs No. of Pages No. of Total Tokens\\n\\nCDIP DocBank Total\\n\\n5,092,636 499,609 5,592,245\\n\\n16,293,353 499,609 16,792,962\\n\\n3,637,551,478 228,362,274 3,865,913,752\\n\\nTable 3: Instruction-tuning dataset statistics. Tasks No. of Training No. of Testing\\n\\nVQA NLI KIE CLS Total\\n\\n145,090 104,360 236,806 149,627 635,883\\n\\n24,347 12,720 38,039 21,813 96,919', metadata={'source': 'example_data/2401.00908.pdf'}), 0.00019343123), (Document(page_content='\\n\\nSame Tasks, Different Datasets (STDD): Following [40, 41, 60, 61], we also evaluate DocLLM on held-out datasets. More precisely, we instruction-tune the pre-trained checkpoint of DocLLM on prompts from 11 of the 16 datasets considered in SDDS, then evaluate DocLLM on the test split of the remaining three datasets. The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged but domains and layouts differ from train to test. We believe examining this setting in the DocAI field is relevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS, while document characteristics tend to change more often in production. We specifically isolate DocVQA, KLC, and BizDocs for STDD evaluation in order to (1) exclude at least one dataset per task from SFT when possible, (2) leave enough datapoints per task in the training split of the instruction-tuning data, (3) avoid data leakage (certain datasets were obtained from the same sources), and (4) benchmark models on popular yet challenging datasets when possible. Due to the high cost of instruction-tuning, we were not able to run additional experiments with different held-out datasets.', metadata={'source': 'example_data/2401.00908.pdf'}), 8.96884e-05), (Document(page_content='We have introduced the datasets used to conduct instruction tuning on Section 3.4. These datasets encompass four common DocAI tasks: VQA, NLI, KIE, and CLS. Note that when a prompt includes a list of possible answers, we create multiple copies of the prompt with one possible answer assigned to each. We only perform this “flattening” operation in the training split of the dataset. Detailed statistics for these tasks are presented in Table 3.\\n\\n4.2 Model Setup and Training Details', metadata={'source': 'example_data/2401.00908.pdf'}), 8.125545e-05)]\n",
      "Reranked document:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "3 Dataset Overview\n",
      "\n",
      "This study evaluates fine-tuned and RAG-enhanced language models using context-related questions and answers originated datasets from three major crop producer countries: USA, Brazil and India. In our case, we are using agriculture as the industrial setting as an example. Available data varied considerably in format and content, ranging from regulatory documents, scientific reports, agronomic exams, to knowledge databases. In this section, we present each dataset in more detail.\n",
      "\n",
      "3.1 USA\n",
      "----------------------------------------------------------------------------------------------------\n",
      "metadata: {'source': 'example_data/2401.08406.pdf'}\n",
      "==================================================================================================== \n",
      "\n",
      "\n",
      "Example 5: Query-> Where was the agriculture dataset collected for the USA?\n",
      "....................................................................................................\n",
      "Retrieved document:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "10 [Document(page_content='3 Dataset Overview\\n\\nThis study evaluates fine-tuned and RAG-enhanced language models using context-related questions and answers originated datasets from three major crop producer countries: USA, Brazil and India. In our case, we are using agriculture as the industrial setting as an example. Available data varied considerably in format and content, ranging from regulatory documents, scientific reports, agronomic exams, to knowledge databases. In this section, we present each dataset in more detail.\\n\\n3.1 USA', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='3.1 USA\\n\\nWe collected documents, handbooks, and reports publicly available online from the United States Department of Agriculture (USDA), state agriculture and consumer services agencies, and partners from the Land-Grant Institutions National Program. Available documents contain federal regulatory and policy information surrounding crop and livestock management, information on diseases and best practices, quality assurance and export regulations, details on assistance programs, as well as insurance and pricing guidelines. Collected data totals more than 23k PDF files with over 50M tokens, representing 44 states in the USA. We downloaded and preprocessed these files, extracting the textual information that could be used as input to the Q&A generation pipeline. To benchmark and evaluate the models, we employed the documents related to the Washington state, which comprises 573 files with over 2M tokens. We present in Listing 5 an example of content within these documents.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='acres, forced U.S. apple growers to also market these varieties domestically.\\n\\n...\\n\\nListing 5: Example of a passage in one of the Washington state documents, focusing on the production of Gala apples.\\n\\n3.2 Brazil\\n\\nWe used a dataset called \"500 Questions 500 Answers - Embrapa/SCT\" containing books with questions and answers related to various aspects of crop cultivation and management in Brazil. The questions are text-based inquiries formulated by a diverse group of stakeholders, including producers, farmers, and farming associations, and are accompanied by responses from Embrapa specialists.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='The dataset, \"500 Questions 500 Answers - Embrapa/SCT\" provides a valuable resource for farmers and agronomists to improve their knowledge of various agricultural topics. The questions in the dataset are not presented in a multiple- choice format, which requires a deeper understanding of the subject matter to generate accurate and relevant answers. Although the questions often have no single correct answer, the dataset encompasses a wide range of topics essential to both groups. The study highlighted the importance of understanding the nuances of agricultural topics and the expertise required to evaluate the relevance and correctness of AI-generated answers. The Embrapa dataset aims to foster sustainable agricultural practices and contribute to the overall knowledge base.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='The remainder of this work is organized as follows. Section 2 presents the methodology in detail, including the data acquisition process, information extraction procedure, question and answer generation, and fine-tuning of the model. We then describe in Section 3 the dataset used in the study, which includes data from the USA, Brazil, and India. In Section 4, we outline the metrics used to evaluate the effectiveness of the proposed methodology, focusing on both question and answer evaluation. Section 5 presents a comprehensive evaluation of various models and their performance in generating question-answer pairs within the context of agricultural data, using these for RAG on GPT-4, Vicuna (Zheng et al., 2023) and Llama2 13B (Touvron et al., 2023a,b) and fine-tuning of GPT-4 and Llama2 13B. Finally, we conclude with a summary of the main findings and possible directions for future research in Section 6.\\n\\n2 Methodology', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='The proposed pipeline aims to generate domain-specific questions and answers catering to professionals and stakeholders in an industry where answers from a copilot are expected to be grounded by relevant industry-specific factors. In the case of our agriculture study, we are aiming to generate geography-specific answers. For this, our starting point is an agriculture dataset, which is fed into three main components: Q&A generation, Retrieval-Augmented Generation (RAG) (Lewis et al., 2020), and fine-tuning process. The Q&A generation creates question and answer pairs based on the information available in the agriculture dataset, while RAG uses it as a knowledge source. The generated data is then refined and used to fine-tune several models while their quality is evaluated using a combination of proposed metrics. Through this comprehensive approach, we aim to harness the power of LLMs for the benefit of the agriculture industry and its stakeholders.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='Comprehensive evaluation of LLMs: we conducted an extensive evaluation of large language models, including LlaMa2-13B, GPT-4 and Vicuna (Zheng et al., 2023), in answering agriculture-related questions. This was done using benchmark datasets from major agriculture producer countries. Our evaluation included the complete fine-tuning and RAG pipeline, each with its own set of metrics. The findings from this evaluation provide a crucial baseline understanding of the performance of these models within the agricultural context. Moreover, we conduct evaluations that demonstrate the impact of spatial shift on the knowledge encoded by existing LMs and the improvements offered by spatially-scoped fine-tuning. In our analysis, GPT-4 consistently outperformed other models, however the cost associated with its fine-tuning and inference needs to be taken into consideration.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='\\n\\nDespite these advancements, the application of AI in specific fields such as agriculture is still limited due to a lack of specialized training data. While AI has been used to derive insights from satellite imagery and sensor data in agriculture (Vasisht et al., 2017; Chandra et al., 2022; Microsoft, 2021; Zhao et al., 2023; Kumar et al., 2021; Sharma et al., 2023), the technology is still slowly being adopted by farmers. While GPT-4 and Bing are powerful tools for finding information, they may not provide the best solutions for farmers who have very specific questions about their crops and livestock. These questions often require knowledge of local conditions, specific varieties, and up-to-date data that might not be readily available through general search engines (Silva et al., 2023). As an example, Table 1 compares the answers from GPT-4 and an agronomist expert to the same query asked for three different U.S. states. While an expert would provide contextualized answers grounded on the states specific climate and agriculture tradition, LLMs provide a generic answer that, although correct, is not as precise for each state as the expert answer.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='Answer generation: with the question and the retrieved chunks as input, an LLM model was used to synthesize an answer. Specifically, we provided the retrieved information from the FAISS database to GPT-4 as context within a custom prompt, which allowed the generation of domain-specific answers. The answers were properly formatted in a JSON file along with the associated questions to create the Q&A pair.\\n\\n9\\n\\n{{#system~}} You are an expert in agriculture and you are formulating questions from documents to assess\\n\\nthe knowledge of a student about agriculture-related topics.\\n\\nYou have access to the following document metadata encoded as JSON: {{context}} {{~/system}}\\n\\n{{#user~}} An example of expected output follows:\\n\\nExamples: {examples}\\n\\nThe document is from {{source}} and has title: {{title}}\\n\\nAdd location (state, country) and crop information to each question, if possible.\\n\\nPlease formulate as many questions as possible to assess knowledge of the text below.\\n\\n{{section}}\\n\\n{{~/user}}', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='2.1 Data Acquisition\\n\\nThe initial focus of the pipeline is to gather an assorted and well-curated dataset that captures information of interest to an industry. This enables the generation of questions and answers, forming the foundation for refining the models to produce more precise and pertinent responses. For this step, we seek data sources that contain high-quality, authoritative information on the topic of interest. For example, in agriculture, this includes agricultural and environmental government agencies, scientific knowledge repositories, and agronomist exams databases. It is also important that the information extracted to be aligned with the grounding that will be provided to the model. For instance, in the case of agricultural data, we sourced guidelines and procedures that were geography-specific, i.e. with a shared location among documents. We provide further details on the sources and type of documents selected for this step in Section 3.', metadata={'source': 'example_data/2401.08406.pdf'})]\n",
      "Reranked documents:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "10 [(Document(page_content='3 Dataset Overview\\n\\nThis study evaluates fine-tuned and RAG-enhanced language models using context-related questions and answers originated datasets from three major crop producer countries: USA, Brazil and India. In our case, we are using agriculture as the industrial setting as an example. Available data varied considerably in format and content, ranging from regulatory documents, scientific reports, agronomic exams, to knowledge databases. In this section, we present each dataset in more detail.\\n\\n3.1 USA', metadata={'source': 'example_data/2401.08406.pdf'}), 0.14175667), (Document(page_content='3.1 USA\\n\\nWe collected documents, handbooks, and reports publicly available online from the United States Department of Agriculture (USDA), state agriculture and consumer services agencies, and partners from the Land-Grant Institutions National Program. Available documents contain federal regulatory and policy information surrounding crop and livestock management, information on diseases and best practices, quality assurance and export regulations, details on assistance programs, as well as insurance and pricing guidelines. Collected data totals more than 23k PDF files with over 50M tokens, representing 44 states in the USA. We downloaded and preprocessed these files, extracting the textual information that could be used as input to the Q&A generation pipeline. To benchmark and evaluate the models, we employed the documents related to the Washington state, which comprises 573 files with over 2M tokens. We present in Listing 5 an example of content within these documents.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.0076592145), (Document(page_content='The remainder of this work is organized as follows. Section 2 presents the methodology in detail, including the data acquisition process, information extraction procedure, question and answer generation, and fine-tuning of the model. We then describe in Section 3 the dataset used in the study, which includes data from the USA, Brazil, and India. In Section 4, we outline the metrics used to evaluate the effectiveness of the proposed methodology, focusing on both question and answer evaluation. Section 5 presents a comprehensive evaluation of various models and their performance in generating question-answer pairs within the context of agricultural data, using these for RAG on GPT-4, Vicuna (Zheng et al., 2023) and Llama2 13B (Touvron et al., 2023a,b) and fine-tuning of GPT-4 and Llama2 13B. Finally, we conclude with a summary of the main findings and possible directions for future research in Section 6.\\n\\n2 Methodology', metadata={'source': 'example_data/2401.08406.pdf'}), 0.0018513764), (Document(page_content='2.1 Data Acquisition\\n\\nThe initial focus of the pipeline is to gather an assorted and well-curated dataset that captures information of interest to an industry. This enables the generation of questions and answers, forming the foundation for refining the models to produce more precise and pertinent responses. For this step, we seek data sources that contain high-quality, authoritative information on the topic of interest. For example, in agriculture, this includes agricultural and environmental government agencies, scientific knowledge repositories, and agronomist exams databases. It is also important that the information extracted to be aligned with the grounding that will be provided to the model. For instance, in the case of agricultural data, we sourced guidelines and procedures that were geography-specific, i.e. with a shared location among documents. We provide further details on the sources and type of documents selected for this step in Section 3.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.0013381068), (Document(page_content='acres, forced U.S. apple growers to also market these varieties domestically.\\n\\n...\\n\\nListing 5: Example of a passage in one of the Washington state documents, focusing on the production of Gala apples.\\n\\n3.2 Brazil\\n\\nWe used a dataset called \"500 Questions 500 Answers - Embrapa/SCT\" containing books with questions and answers related to various aspects of crop cultivation and management in Brazil. The questions are text-based inquiries formulated by a diverse group of stakeholders, including producers, farmers, and farming associations, and are accompanied by responses from Embrapa specialists.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.001038961), (Document(page_content='Answer generation: with the question and the retrieved chunks as input, an LLM model was used to synthesize an answer. Specifically, we provided the retrieved information from the FAISS database to GPT-4 as context within a custom prompt, which allowed the generation of domain-specific answers. The answers were properly formatted in a JSON file along with the associated questions to create the Q&A pair.\\n\\n9\\n\\n{{#system~}} You are an expert in agriculture and you are formulating questions from documents to assess\\n\\nthe knowledge of a student about agriculture-related topics.\\n\\nYou have access to the following document metadata encoded as JSON: {{context}} {{~/system}}\\n\\n{{#user~}} An example of expected output follows:\\n\\nExamples: {examples}\\n\\nThe document is from {{source}} and has title: {{title}}\\n\\nAdd location (state, country) and crop information to each question, if possible.\\n\\nPlease formulate as many questions as possible to assess knowledge of the text below.\\n\\n{{section}}\\n\\n{{~/user}}', metadata={'source': 'example_data/2401.08406.pdf'}), 0.0009943192), (Document(page_content='\\n\\nDespite these advancements, the application of AI in specific fields such as agriculture is still limited due to a lack of specialized training data. While AI has been used to derive insights from satellite imagery and sensor data in agriculture (Vasisht et al., 2017; Chandra et al., 2022; Microsoft, 2021; Zhao et al., 2023; Kumar et al., 2021; Sharma et al., 2023), the technology is still slowly being adopted by farmers. While GPT-4 and Bing are powerful tools for finding information, they may not provide the best solutions for farmers who have very specific questions about their crops and livestock. These questions often require knowledge of local conditions, specific varieties, and up-to-date data that might not be readily available through general search engines (Silva et al., 2023). As an example, Table 1 compares the answers from GPT-4 and an agronomist expert to the same query asked for three different U.S. states. While an expert would provide contextualized answers grounded on the states specific climate and agriculture tradition, LLMs provide a generic answer that, although correct, is not as precise for each state as the expert answer.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.00073098036), (Document(page_content='The dataset, \"500 Questions 500 Answers - Embrapa/SCT\" provides a valuable resource for farmers and agronomists to improve their knowledge of various agricultural topics. The questions in the dataset are not presented in a multiple- choice format, which requires a deeper understanding of the subject matter to generate accurate and relevant answers. Although the questions often have no single correct answer, the dataset encompasses a wide range of topics essential to both groups. The study highlighted the importance of understanding the nuances of agricultural topics and the expertise required to evaluate the relevance and correctness of AI-generated answers. The Embrapa dataset aims to foster sustainable agricultural practices and contribute to the overall knowledge base.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.0007107614), (Document(page_content='The proposed pipeline aims to generate domain-specific questions and answers catering to professionals and stakeholders in an industry where answers from a copilot are expected to be grounded by relevant industry-specific factors. In the case of our agriculture study, we are aiming to generate geography-specific answers. For this, our starting point is an agriculture dataset, which is fed into three main components: Q&A generation, Retrieval-Augmented Generation (RAG) (Lewis et al., 2020), and fine-tuning process. The Q&A generation creates question and answer pairs based on the information available in the agriculture dataset, while RAG uses it as a knowledge source. The generated data is then refined and used to fine-tune several models while their quality is evaluated using a combination of proposed metrics. Through this comprehensive approach, we aim to harness the power of LLMs for the benefit of the agriculture industry and its stakeholders.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.00046863305), (Document(page_content='Comprehensive evaluation of LLMs: we conducted an extensive evaluation of large language models, including LlaMa2-13B, GPT-4 and Vicuna (Zheng et al., 2023), in answering agriculture-related questions. This was done using benchmark datasets from major agriculture producer countries. Our evaluation included the complete fine-tuning and RAG pipeline, each with its own set of metrics. The findings from this evaluation provide a crucial baseline understanding of the performance of these models within the agricultural context. Moreover, we conduct evaluations that demonstrate the impact of spatial shift on the knowledge encoded by existing LMs and the improvements offered by spatially-scoped fine-tuning. In our analysis, GPT-4 consistently outperformed other models, however the cost associated with its fine-tuning and inference needs to be taken into consideration.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.000106043604)]\n",
      "Reranked document:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "3 Dataset Overview\n",
      "\n",
      "This study evaluates fine-tuned and RAG-enhanced language models using context-related questions and answers originated datasets from three major crop producer countries: USA, Brazil and India. In our case, we are using agriculture as the industrial setting as an example. Available data varied considerably in format and content, ranging from regulatory documents, scientific reports, agronomic exams, to knowledge databases. In this section, we present each dataset in more detail.\n",
      "\n",
      "3.1 USA\n",
      "----------------------------------------------------------------------------------------------------\n",
      "metadata: {'source': 'example_data/2401.08406.pdf'}\n",
      "==================================================================================================== \n",
      "\n",
      "\n",
      "Example 6: Query-> What was the answer generation process used in the paper?\n",
      "....................................................................................................\n",
      "Retrieved document:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "10 [Document(page_content='The RAG pipeline begins by retrieving, for a given question, the most relevant documents or passages from our dataset. The retrieval system employs techniques such as BM25, Dense Retrieval (Reimers and Gurevych, 2019; Ni et al., 2022), and other advanced retrieval mechanisms. The retrieved documents serve as a knowledge source for the subsequent generation phase. Once the relevant passages have been identified, the generation component comes into play. An LLM takes the question and the retrieved information as inputs and generates a contextually appropriate answer. The generation process is guided by the context provided by the retrieved documents, ensuring that the generated Q&A pairs are accurate, relevant, and informative. More specifically, the generation process follows three steps:', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='In this study, we also demonstrated how to generate relevant question and answers for datasets for specific industries, by leveraging structured document understanding, together with GPT-4 for question generation, and RAG for answer generation. The questions generated were highly specific to the respective sections they were derived from, and the model was able to utilize the entire text for generating insightful and comprehensive answers. Our exploration indicated that generating questions and answers separately leads to efficient token usage, opening up the possibility of using different models or approaches for each component of the Q&A pair. We also proposed a series of metrics to properly\\n\\n29\\n\\nevaluate the quality of the generated questions relative to the information contained in the original documents, and showcased multiple metrics for measuring the quality of the RAG-generated answers.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='Following data acquisition, the pipeline proceeds to extract information from the collected documents. This step is crucial as it involves parsing complex and unstructured PDF files to recover the underlying content and structure. This process, detailed further in Section 2.2, employs robust text extraction tools and machine learning algorithms to recover textual, tabular, and visual information, while also identifying the semantic structure of the documents and possible cross-relations in them.\\n\\nThe next component of the pipeline is question and answer generation. The objective here is to generate contextually grounded and high-quality questions that accurately reflect the content of the extracted text. The methodology employs a framework to control the structural composition of both inputs and outputs, thereby augmenting the overall efficacy of response generation from language models. This part of the process is detailed in Section 2.3.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='In terms of Relevance, the combined generation method outperforms the only questions method in all contexts. This suggests that generating both questions and answers produces content that is more relevant to the context, when considering a farmer perspective. On the other hand, we tested changing the prompt used to calculate Relevance and removed the farmer perspective. With that, the score of both setups was extremely similar. Lastly, for Fluency, which measures the readability and naturalness of the generated content, both methods perform similarly across all contexts, indicating that both methods generate highly fluent content.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='Subsequently, the pipeline generates answers for the formulated questions. The methodology employed here leverages Retrieval-Augmented Generation (RAG), which combines the power of retrieval and generation mechanisms to create high-quality answers. The answer generation process is discussed further in Section 2.4.\\n\\nFinally, the pipeline fine-tunes the models with the Q&A pairs. The optimization process, discussed in the Section 2.5, employs methods like Low Rank Adaptation (LoRA) (Hu et al., 2021) and ensures a comprehensive understanding of the content and context of the scientific literature, making it a valuable resource for various domains or industries.\\n\\nIn the following sections, we will delve deeper in each components of the pipeline, highlighting their objectives, inputs and outputs, and reasoning behind why they were added to the pipeline.\\n\\n2.1 Data Acquisition', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='Answer generation: with the question and the retrieved chunks as input, an LLM model was used to synthesize an answer. Specifically, we provided the retrieved information from the FAISS database to GPT-4 as context within a custom prompt, which allowed the generation of domain-specific answers. The answers were properly formatted in a JSON file along with the associated questions to create the Q&A pair.\\n\\n9\\n\\n{{#system~}} You are an expert in agriculture and you are formulating questions from documents to assess\\n\\nthe knowledge of a student about agriculture-related topics.\\n\\nYou have access to the following document metadata encoded as JSON: {{context}} {{~/system}}\\n\\n{{#user~}} An example of expected output follows:\\n\\nExamples: {examples}\\n\\nThe document is from {{source}} and has title: {{title}}\\n\\nAdd location (state, country) and crop information to each question, if possible.\\n\\nPlease formulate as many questions as possible to assess knowledge of the text below.\\n\\n{{section}}\\n\\n{{~/user}}', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='\\n\\nThe initial focus of this section of the pipeline is to manage the inherent complexity and variability of natural language when generating questions from the extracted text. We aim to generate contextually grounded and high-quality questions that accurately reflect the content of the extracted text. For this, we employ the Guidance framework (Gui, 2023), whose primary advantage lies in its capacity to provide unparalleled control over the structural composition of both inputs and outputs, thereby augmenting the overall efficacy of response generation from language models. This degree of control results in outputs that are not only more precise, but also exhibit enhanced coherence and contextual relevance. The framework’s capability to amalgamate generation, prompting, and logical control into a singular, unified process closely parallels the inherent mechanisms of language model text processing. Moreover, the unique feature of Guidance that enables the direction of language models via context-specific prompts, contributes to a heightened level of semantic relevance in the resultant text. In our case, this ensures the questions will carry semantic relevance to the source text while taking into account the context extracted from the JSON files.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='The remainder of this work is organized as follows. Section 2 presents the methodology in detail, including the data acquisition process, information extraction procedure, question and answer generation, and fine-tuning of the model. We then describe in Section 3 the dataset used in the study, which includes data from the USA, Brazil, and India. In Section 4, we outline the metrics used to evaluate the effectiveness of the proposed methodology, focusing on both question and answer evaluation. Section 5 presents a comprehensive evaluation of various models and their performance in generating question-answer pairs within the context of agricultural data, using these for RAG on GPT-4, Vicuna (Zheng et al., 2023) and Llama2 13B (Touvron et al., 2023a,b) and fine-tuning of GPT-4 and Llama2 13B. Finally, we conclude with a summary of the main findings and possible directions for future research in Section 6.\\n\\n2 Methodology', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='\\n\\nIn the following experiments, we employed three LLMs, namely GPT-3, GPT-3.5 and GPT-4, to evaluate the quality of generated Q&A pairs. The pairs were generated under different contexts: No context, Context, and External context as explained in the next subsection 5.1.1. We used several metrics (defined on Section 4.1) to assess their quality, including Relevance, Global Relevance, Coverage, Overlap, Diversity, Details, and Fluency. The LLMs assessed the Q&A pairs on various metrics, scoring each on a scale from 1 to 5. For certain metrics such as Overlap and Diversity, we incorporated intricate methods like Kullback-Leibler (KL) divergence and Word Mover’s Distance (WMD) to measure the semantic similarity between the source text and the questions generated. The primary objective of this experimental setup was to comprehend how different Q&A generation techniques can influence the quality of the resulting Q&A pairs. We aimed to discern how various metrics perceive and gauge the quality of these pairs and to highlight any significant disparities in their evaluations. This insight is crucial for refining the Q&A generation process, as it ensures that the resulting content is informative, pertinent, diverse, and fluent.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='The proposed pipeline aims to generate domain-specific questions and answers catering to professionals and stakeholders in an industry where answers from a copilot are expected to be grounded by relevant industry-specific factors. In the case of our agriculture study, we are aiming to generate geography-specific answers. For this, our starting point is an agriculture dataset, which is fed into three main components: Q&A generation, Retrieval-Augmented Generation (RAG) (Lewis et al., 2020), and fine-tuning process. The Q&A generation creates question and answer pairs based on the information available in the agriculture dataset, while RAG uses it as a knowledge source. The generated data is then refined and used to fine-tune several models while their quality is evaluated using a combination of proposed metrics. Through this comprehensive approach, we aim to harness the power of LLMs for the benefit of the agriculture industry and its stakeholders.', metadata={'source': 'example_data/2401.08406.pdf'})]\n",
      "Reranked documents:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "10 [(Document(page_content='Answer generation: with the question and the retrieved chunks as input, an LLM model was used to synthesize an answer. Specifically, we provided the retrieved information from the FAISS database to GPT-4 as context within a custom prompt, which allowed the generation of domain-specific answers. The answers were properly formatted in a JSON file along with the associated questions to create the Q&A pair.\\n\\n9\\n\\n{{#system~}} You are an expert in agriculture and you are formulating questions from documents to assess\\n\\nthe knowledge of a student about agriculture-related topics.\\n\\nYou have access to the following document metadata encoded as JSON: {{context}} {{~/system}}\\n\\n{{#user~}} An example of expected output follows:\\n\\nExamples: {examples}\\n\\nThe document is from {{source}} and has title: {{title}}\\n\\nAdd location (state, country) and crop information to each question, if possible.\\n\\nPlease formulate as many questions as possible to assess knowledge of the text below.\\n\\n{{section}}\\n\\n{{~/user}}', metadata={'source': 'example_data/2401.08406.pdf'}), 0.11539704), (Document(page_content='Subsequently, the pipeline generates answers for the formulated questions. The methodology employed here leverages Retrieval-Augmented Generation (RAG), which combines the power of retrieval and generation mechanisms to create high-quality answers. The answer generation process is discussed further in Section 2.4.\\n\\nFinally, the pipeline fine-tunes the models with the Q&A pairs. The optimization process, discussed in the Section 2.5, employs methods like Low Rank Adaptation (LoRA) (Hu et al., 2021) and ensures a comprehensive understanding of the content and context of the scientific literature, making it a valuable resource for various domains or industries.\\n\\nIn the following sections, we will delve deeper in each components of the pipeline, highlighting their objectives, inputs and outputs, and reasoning behind why they were added to the pipeline.\\n\\n2.1 Data Acquisition', metadata={'source': 'example_data/2401.08406.pdf'}), 0.019054255), (Document(page_content='Following data acquisition, the pipeline proceeds to extract information from the collected documents. This step is crucial as it involves parsing complex and unstructured PDF files to recover the underlying content and structure. This process, detailed further in Section 2.2, employs robust text extraction tools and machine learning algorithms to recover textual, tabular, and visual information, while also identifying the semantic structure of the documents and possible cross-relations in them.\\n\\nThe next component of the pipeline is question and answer generation. The objective here is to generate contextually grounded and high-quality questions that accurately reflect the content of the extracted text. The methodology employs a framework to control the structural composition of both inputs and outputs, thereby augmenting the overall efficacy of response generation from language models. This part of the process is detailed in Section 2.3.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.0029506201), (Document(page_content='In this study, we also demonstrated how to generate relevant question and answers for datasets for specific industries, by leveraging structured document understanding, together with GPT-4 for question generation, and RAG for answer generation. The questions generated were highly specific to the respective sections they were derived from, and the model was able to utilize the entire text for generating insightful and comprehensive answers. Our exploration indicated that generating questions and answers separately leads to efficient token usage, opening up the possibility of using different models or approaches for each component of the Q&A pair. We also proposed a series of metrics to properly\\n\\n29\\n\\nevaluate the quality of the generated questions relative to the information contained in the original documents, and showcased multiple metrics for measuring the quality of the RAG-generated answers.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.0023969463), (Document(page_content='In terms of Relevance, the combined generation method outperforms the only questions method in all contexts. This suggests that generating both questions and answers produces content that is more relevant to the context, when considering a farmer perspective. On the other hand, we tested changing the prompt used to calculate Relevance and removed the farmer perspective. With that, the score of both setups was extremely similar. Lastly, for Fluency, which measures the readability and naturalness of the generated content, both methods perform similarly across all contexts, indicating that both methods generate highly fluent content.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.0015698194), (Document(page_content='The remainder of this work is organized as follows. Section 2 presents the methodology in detail, including the data acquisition process, information extraction procedure, question and answer generation, and fine-tuning of the model. We then describe in Section 3 the dataset used in the study, which includes data from the USA, Brazil, and India. In Section 4, we outline the metrics used to evaluate the effectiveness of the proposed methodology, focusing on both question and answer evaluation. Section 5 presents a comprehensive evaluation of various models and their performance in generating question-answer pairs within the context of agricultural data, using these for RAG on GPT-4, Vicuna (Zheng et al., 2023) and Llama2 13B (Touvron et al., 2023a,b) and fine-tuning of GPT-4 and Llama2 13B. Finally, we conclude with a summary of the main findings and possible directions for future research in Section 6.\\n\\n2 Methodology', metadata={'source': 'example_data/2401.08406.pdf'}), 0.0013724556), (Document(page_content='The RAG pipeline begins by retrieving, for a given question, the most relevant documents or passages from our dataset. The retrieval system employs techniques such as BM25, Dense Retrieval (Reimers and Gurevych, 2019; Ni et al., 2022), and other advanced retrieval mechanisms. The retrieved documents serve as a knowledge source for the subsequent generation phase. Once the relevant passages have been identified, the generation component comes into play. An LLM takes the question and the retrieved information as inputs and generates a contextually appropriate answer. The generation process is guided by the context provided by the retrieved documents, ensuring that the generated Q&A pairs are accurate, relevant, and informative. More specifically, the generation process follows three steps:', metadata={'source': 'example_data/2401.08406.pdf'}), 0.0008731826), (Document(page_content='\\n\\nThe initial focus of this section of the pipeline is to manage the inherent complexity and variability of natural language when generating questions from the extracted text. We aim to generate contextually grounded and high-quality questions that accurately reflect the content of the extracted text. For this, we employ the Guidance framework (Gui, 2023), whose primary advantage lies in its capacity to provide unparalleled control over the structural composition of both inputs and outputs, thereby augmenting the overall efficacy of response generation from language models. This degree of control results in outputs that are not only more precise, but also exhibit enhanced coherence and contextual relevance. The framework’s capability to amalgamate generation, prompting, and logical control into a singular, unified process closely parallels the inherent mechanisms of language model text processing. Moreover, the unique feature of Guidance that enables the direction of language models via context-specific prompts, contributes to a heightened level of semantic relevance in the resultant text. In our case, this ensures the questions will carry semantic relevance to the source text while taking into account the context extracted from the JSON files.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.00020804825), (Document(page_content='The proposed pipeline aims to generate domain-specific questions and answers catering to professionals and stakeholders in an industry where answers from a copilot are expected to be grounded by relevant industry-specific factors. In the case of our agriculture study, we are aiming to generate geography-specific answers. For this, our starting point is an agriculture dataset, which is fed into three main components: Q&A generation, Retrieval-Augmented Generation (RAG) (Lewis et al., 2020), and fine-tuning process. The Q&A generation creates question and answer pairs based on the information available in the agriculture dataset, while RAG uses it as a knowledge source. The generated data is then refined and used to fine-tune several models while their quality is evaluated using a combination of proposed metrics. Through this comprehensive approach, we aim to harness the power of LLMs for the benefit of the agriculture industry and its stakeholders.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.00016865178), (Document(page_content='\\n\\nIn the following experiments, we employed three LLMs, namely GPT-3, GPT-3.5 and GPT-4, to evaluate the quality of generated Q&A pairs. The pairs were generated under different contexts: No context, Context, and External context as explained in the next subsection 5.1.1. We used several metrics (defined on Section 4.1) to assess their quality, including Relevance, Global Relevance, Coverage, Overlap, Diversity, Details, and Fluency. The LLMs assessed the Q&A pairs on various metrics, scoring each on a scale from 1 to 5. For certain metrics such as Overlap and Diversity, we incorporated intricate methods like Kullback-Leibler (KL) divergence and Word Mover’s Distance (WMD) to measure the semantic similarity between the source text and the questions generated. The primary objective of this experimental setup was to comprehend how different Q&A generation techniques can influence the quality of the resulting Q&A pairs. We aimed to discern how various metrics perceive and gauge the quality of these pairs and to highlight any significant disparities in their evaluations. This insight is crucial for refining the Q&A generation process, as it ensures that the resulting content is informative, pertinent, diverse, and fluent.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.00016206945)]\n",
      "Reranked document:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Answer generation: with the question and the retrieved chunks as input, an LLM model was used to synthesize an answer. Specifically, we provided the retrieved information from the FAISS database to GPT-4 as context within a custom prompt, which allowed the generation of domain-specific answers. The answers were properly formatted in a JSON file along with the associated questions to create the Q&A pair.\n",
      "\n",
      "9\n",
      "\n",
      "{{#system~}} You are an expert in agriculture and you are formulating questions from documents to assess\n",
      "\n",
      "the knowledge of a student about agriculture-related topics.\n",
      "\n",
      "You have access to the following document metadata encoded as JSON: {{context}} {{~/system}}\n",
      "\n",
      "{{#user~}} An example of expected output follows:\n",
      "\n",
      "Examples: {examples}\n",
      "\n",
      "The document is from {{source}} and has title: {{title}}\n",
      "\n",
      "Add location (state, country) and crop information to each question, if possible.\n",
      "\n",
      "Please formulate as many questions as possible to assess knowledge of the text below.\n",
      "\n",
      "{{section}}\n",
      "\n",
      "{{~/user}}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "metadata: {'source': 'example_data/2401.08406.pdf'}\n",
      "==================================================================================================== \n",
      "\n",
      "\n",
      "Example 7: Query-> how was the content and structure of available documents augmented?\n",
      "....................................................................................................\n",
      "Retrieved document:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "10 [Document(page_content='From the GROBID-generated TEI files, we extracted a subset of the sections of the TEI files comprising the document metadata (title, authors, abstract), sections, tables, figure references, bibliography, and the content itself. Crucially, this phase underscores the belief that the structure of the text is as important as its content. The final objective is to convert the TEI files into more manageable JSON files that preserve not only the content, but also the structure of the original\\n\\n5\\n\\nFigure 2: Example of a document from Washington state present in our dataset. The diverse layouts of PDF files, which often include textual and visual data, pose a significant challenge in terms of extracting not just the content, but also the underlying structure.\\n\\n6\\n\\nAdvances in Dryland Farming in the Inland Pacific Northwest Georgine Yorgey and Chad Kruger, editorsFor Sanford Eigenbrode, in recognition of his resolute\\n\\neffort', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='Following data acquisition, the pipeline proceeds to extract information from the collected documents. This step is crucial as it involves parsing complex and unstructured PDF files to recover the underlying content and structure. This process, detailed further in Section 2.2, employs robust text extraction tools and machine learning algorithms to recover textual, tabular, and visual information, while also identifying the semantic structure of the documents and possible cross-relations in them.\\n\\nThe next component of the pipeline is question and answer generation. The objective here is to generate contextually grounded and high-quality questions that accurately reflect the content of the extracted text. The methodology employs a framework to control the structural composition of both inputs and outputs, thereby augmenting the overall efficacy of response generation from language models. This part of the process is detailed in Section 2.3.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='With this in mind, the main objective of this step of the pipeline is to address the complexities inherent in processing data derived from a range of formatted PDF documents. This is achieved by leveraging robust text extraction tools and machine learning algorithms that employ advanced natural language processing techniques. The focus is not only in recovering the content of each file, but also its structure. Among other things, we are interested in discovering what are the sections and subsections, parsing the information presented in tables and diagrams, identifying cross-references within the document, and linking images with their caption and description. By retrieving the organization of the document, we can easily group information, reason over numerical data present in tables, and provide more consistent snippets of the text to the Q&A generation step. It is also very important that all available information is extracted from the document, with well-formed sentences.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='First, we augment the content and structure of available documents by explicitly adding supporting tags from the text. We formulated prompts to extract a list of locations and agronomic topics mentioned in each section of the document (e.g., if that section refers to crops, cattle, or diseases), as exemplified in Listing 3, and task the LLM model to answering them based on the data extracted from the JSON files. The aim is to use of the the additional information, including locations and mentioned topics, to ground the generation process, enhancing the relevance of the questions and guiding the model to cover a wide range of topics and challenges.\\n\\n7\\n\\n{\\n\\n\"grobid_version\": \"0.7.3\", \"grobid_timestamp\": \"2023-07-04T13:05+0000\", \"language_code\": \"en\", \"citations\": [\\n\\n{\\n\\n\"authors\": [\\n\\n{\\n\\n\"given_name\": \"J\", \"surname\": \"Abatzoglou\", \"name\": \"J T Abatzoglou\"\\n\\n}, {\\n\\n\"given_name\": \"T\", \"surname\": \"Brown\", \"name\": \"T J Brown\"\\n\\n}', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='There are multiple tools available online that extract information from PDFs (PDF2Text, 2023; PyPDF, 2023). However, many of them lack the ability to retrieve content in a structured way. For example, pdf2text is an open-source Python library offering methods to iterate over PDF’s pages and recover the textual information. We provide in Listing 1 the output of pdf2text over the document from Figure 2. The library is able to recover the textual information, but markers representing the beginning of a section or subsection are lost within the retrieved data, hindering our ability to reason over the document structure. Captions of tables and figures are also lost in conversion but sometimes contain critical information for the understanding of the document.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='2.2 PDF Information Extraction\\n\\nIn our study, the extraction of information and text structure from the collected documents is critical to the quality of the subsequent steps. However, this is a challenging task as the primary purpose of PDFs is to accurately display a document across different systems, and not for easy information extraction. The underlying structure of a PDF file does not map onto the logical structure of a document, i.e., sections, subsection, and associated content. Additionally, with documents originating from various sources, we observe their layouts and formatting to be complex and lack standardization, often presenting a mixture of tables, images, sidebars, and page footers. We present in Figure 2 an example of a PDF file in our dataset.', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='We create the templates following what we believe end users would generally ask about documents (Table 1). For KIE and CLS, we hypothesize that (1) the extraction instructions can teach DocLLM to correlate names of keys in the prompts with document fields so as to retrieve values, (2) the internal classification instructions can help the model understand what intrinsically characterizes each key or document type, and (3) the multiple choice question (MCQ) instructions can teach the model to leverage its comprehension of key names included as choices in the prompt (resp. document type names) to classify extracted values (resp. entire documents). We introduce the templates in detail as follows.', metadata={'source': 'example_data/2401.00908.pdf'}), Document(page_content='The major areas of production in the inland\\n\\nListing 1: Text extracted from the PDF presented in Figure 2 using PDF2text. Structural markers, such as the “Introduction” section header, end up lost within the parsed text.\\n\\nPDFs. This approach ensures a comprehensive understanding of the content and context of the scientific literature, making it a valuable resource for various domains or industries.\\n\\n2.3 Question Generation', metadata={'source': 'example_data/2401.08406.pdf'}), Document(page_content='6 Discussion and Findings\\n\\nIn addition to its immediate utility in visually rich document understanding tasks, we posit that DocLLM offers an opportunity to change the landscape of generative pre-training by enabling language models to go beyond next token prediction in plain text settings. By accommodating complex layout structures, DocLLM allows for e-books, e-publications, and other documents with rich layouts to be incorporated into the pre-training corpus without requiring extensive preprocessing. The spatial-aware reading approach enables the model to perceive the document as inherently structured knowledge.', metadata={'source': 'example_data/2401.00908.pdf'}), Document(page_content='With authoritative sources defined, web scraping tools come into play to gather the required data. We employed web scraping frameworks, including Scrapy (Zyte, 2023) and BeautifulSoup (BeautifulSoup, 2023), to parse through the websites, uncovering all available documents and downloading the relevant files.\\n\\n4\\n\\nFigure 1: Methodology pipeline. Domain-specific datasets are collected, and the content and structure of the documents are extracted. This information is then fed to the Q&A generation step. Synthesized question-answer pairs are used to fine-tune the LLMs. Models are evaluated with and without RAG under different GPT-4-based metrics.\\n\\n2.2 PDF Information Extraction', metadata={'source': 'example_data/2401.08406.pdf'})]\n",
      "Reranked documents:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "10 [(Document(page_content='First, we augment the content and structure of available documents by explicitly adding supporting tags from the text. We formulated prompts to extract a list of locations and agronomic topics mentioned in each section of the document (e.g., if that section refers to crops, cattle, or diseases), as exemplified in Listing 3, and task the LLM model to answering them based on the data extracted from the JSON files. The aim is to use of the the additional information, including locations and mentioned topics, to ground the generation process, enhancing the relevance of the questions and guiding the model to cover a wide range of topics and challenges.\\n\\n7\\n\\n{\\n\\n\"grobid_version\": \"0.7.3\", \"grobid_timestamp\": \"2023-07-04T13:05+0000\", \"language_code\": \"en\", \"citations\": [\\n\\n{\\n\\n\"authors\": [\\n\\n{\\n\\n\"given_name\": \"J\", \"surname\": \"Abatzoglou\", \"name\": \"J T Abatzoglou\"\\n\\n}, {\\n\\n\"given_name\": \"T\", \"surname\": \"Brown\", \"name\": \"T J Brown\"\\n\\n}', metadata={'source': 'example_data/2401.08406.pdf'}), 0.9553296), (Document(page_content='With authoritative sources defined, web scraping tools come into play to gather the required data. We employed web scraping frameworks, including Scrapy (Zyte, 2023) and BeautifulSoup (BeautifulSoup, 2023), to parse through the websites, uncovering all available documents and downloading the relevant files.\\n\\n4\\n\\nFigure 1: Methodology pipeline. Domain-specific datasets are collected, and the content and structure of the documents are extracted. This information is then fed to the Q&A generation step. Synthesized question-answer pairs are used to fine-tune the LLMs. Models are evaluated with and without RAG under different GPT-4-based metrics.\\n\\n2.2 PDF Information Extraction', metadata={'source': 'example_data/2401.08406.pdf'}), 0.035838928), (Document(page_content='From the GROBID-generated TEI files, we extracted a subset of the sections of the TEI files comprising the document metadata (title, authors, abstract), sections, tables, figure references, bibliography, and the content itself. Crucially, this phase underscores the belief that the structure of the text is as important as its content. The final objective is to convert the TEI files into more manageable JSON files that preserve not only the content, but also the structure of the original\\n\\n5\\n\\nFigure 2: Example of a document from Washington state present in our dataset. The diverse layouts of PDF files, which often include textual and visual data, pose a significant challenge in terms of extracting not just the content, but also the underlying structure.\\n\\n6\\n\\nAdvances in Dryland Farming in the Inland Pacific Northwest Georgine Yorgey and Chad Kruger, editorsFor Sanford Eigenbrode, in recognition of his resolute\\n\\neffort', metadata={'source': 'example_data/2401.08406.pdf'}), 0.009048245), (Document(page_content='Following data acquisition, the pipeline proceeds to extract information from the collected documents. This step is crucial as it involves parsing complex and unstructured PDF files to recover the underlying content and structure. This process, detailed further in Section 2.2, employs robust text extraction tools and machine learning algorithms to recover textual, tabular, and visual information, while also identifying the semantic structure of the documents and possible cross-relations in them.\\n\\nThe next component of the pipeline is question and answer generation. The objective here is to generate contextually grounded and high-quality questions that accurately reflect the content of the extracted text. The methodology employs a framework to control the structural composition of both inputs and outputs, thereby augmenting the overall efficacy of response generation from language models. This part of the process is detailed in Section 2.3.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.006844488), (Document(page_content='With this in mind, the main objective of this step of the pipeline is to address the complexities inherent in processing data derived from a range of formatted PDF documents. This is achieved by leveraging robust text extraction tools and machine learning algorithms that employ advanced natural language processing techniques. The focus is not only in recovering the content of each file, but also its structure. Among other things, we are interested in discovering what are the sections and subsections, parsing the information presented in tables and diagrams, identifying cross-references within the document, and linking images with their caption and description. By retrieving the organization of the document, we can easily group information, reason over numerical data present in tables, and provide more consistent snippets of the text to the Q&A generation step. It is also very important that all available information is extracted from the document, with well-formed sentences.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.0062024854), (Document(page_content='2.2 PDF Information Extraction\\n\\nIn our study, the extraction of information and text structure from the collected documents is critical to the quality of the subsequent steps. However, this is a challenging task as the primary purpose of PDFs is to accurately display a document across different systems, and not for easy information extraction. The underlying structure of a PDF file does not map onto the logical structure of a document, i.e., sections, subsection, and associated content. Additionally, with documents originating from various sources, we observe their layouts and formatting to be complex and lack standardization, often presenting a mixture of tables, images, sidebars, and page footers. We present in Figure 2 an example of a PDF file in our dataset.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.0046181763), (Document(page_content='6 Discussion and Findings\\n\\nIn addition to its immediate utility in visually rich document understanding tasks, we posit that DocLLM offers an opportunity to change the landscape of generative pre-training by enabling language models to go beyond next token prediction in plain text settings. By accommodating complex layout structures, DocLLM allows for e-books, e-publications, and other documents with rich layouts to be incorporated into the pre-training corpus without requiring extensive preprocessing. The spatial-aware reading approach enables the model to perceive the document as inherently structured knowledge.', metadata={'source': 'example_data/2401.00908.pdf'}), 0.0019213193), (Document(page_content='There are multiple tools available online that extract information from PDFs (PDF2Text, 2023; PyPDF, 2023). However, many of them lack the ability to retrieve content in a structured way. For example, pdf2text is an open-source Python library offering methods to iterate over PDF’s pages and recover the textual information. We provide in Listing 1 the output of pdf2text over the document from Figure 2. The library is able to recover the textual information, but markers representing the beginning of a section or subsection are lost within the retrieved data, hindering our ability to reason over the document structure. Captions of tables and figures are also lost in conversion but sometimes contain critical information for the understanding of the document.', metadata={'source': 'example_data/2401.08406.pdf'}), 0.0011920377), (Document(page_content='The major areas of production in the inland\\n\\nListing 1: Text extracted from the PDF presented in Figure 2 using PDF2text. Structural markers, such as the “Introduction” section header, end up lost within the parsed text.\\n\\nPDFs. This approach ensures a comprehensive understanding of the content and context of the scientific literature, making it a valuable resource for various domains or industries.\\n\\n2.3 Question Generation', metadata={'source': 'example_data/2401.08406.pdf'}), 0.00065472745), (Document(page_content='We create the templates following what we believe end users would generally ask about documents (Table 1). For KIE and CLS, we hypothesize that (1) the extraction instructions can teach DocLLM to correlate names of keys in the prompts with document fields so as to retrieve values, (2) the internal classification instructions can help the model understand what intrinsically characterizes each key or document type, and (3) the multiple choice question (MCQ) instructions can teach the model to leverage its comprehension of key names included as choices in the prompt (resp. document type names) to classify extracted values (resp. entire documents). We introduce the templates in detail as follows.', metadata={'source': 'example_data/2401.00908.pdf'}), 0.00013907252)]\n",
      "Reranked document:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "First, we augment the content and structure of available documents by explicitly adding supporting tags from the text. We formulated prompts to extract a list of locations and agronomic topics mentioned in each section of the document (e.g., if that section refers to crops, cattle, or diseases), as exemplified in Listing 3, and task the LLM model to answering them based on the data extracted from the JSON files. The aim is to use of the the additional information, including locations and mentioned topics, to ground the generation process, enhancing the relevance of the questions and guiding the model to cover a wide range of topics and challenges.\n",
      "\n",
      "7\n",
      "\n",
      "{\n",
      "\n",
      "\"grobid_version\": \"0.7.3\", \"grobid_timestamp\": \"2023-07-04T13:05+0000\", \"language_code\": \"en\", \"citations\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"authors\": [\n",
      "\n",
      "{\n",
      "\n",
      "\"given_name\": \"J\", \"surname\": \"Abatzoglou\", \"name\": \"J T Abatzoglou\"\n",
      "\n",
      "}, {\n",
      "\n",
      "\"given_name\": \"T\", \"surname\": \"Brown\", \"name\": \"T J Brown\"\n",
      "\n",
      "}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "metadata: {'source': 'example_data/2401.08406.pdf'}\n",
      "==================================================================================================== \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query1 = \"What are the metrics used to evaluate the answers?\"\n",
    "query2 = \"How many pdf data were collected from the USA?\"\n",
    "query3 = \"What is the DocLLM architecture ?\"\n",
    "query4 = \"Which contries were used to collect dataset?\"\n",
    "query5 = \"Where was the agriculture dataset collected for the USA?\"\n",
    "query6 = \"how was the content and structure of available documents augmented?\"\n",
    "query7 = \"What was the answer generation process used in the paper?\"\n",
    "query8 = \"how was the content and structure of available documents augmented?\"\n",
    "\n",
    "queries = [\n",
    "    query1,\n",
    "    query2,\n",
    "    query3,\n",
    "    query4,\n",
    "    query5,\n",
    "    query7,\n",
    "    query8,\n",
    "]\n",
    "\n",
    "for i, query in enumerate(queries):\n",
    "    print(f\"Example {i+1}: Query->\", query)\n",
    "    print(\n",
    "        \"..\" * 50,\n",
    "    )\n",
    "    print(\"Retrieved document:\")\n",
    "\n",
    "    retrieved_documents = retriever.get_relevant_documents(query)\n",
    "    print(\"--\" * 50)\n",
    "    print(len(retrieved_documents),retrieved_documents)\n",
    "    reranked_documents = rerank_docs(reranker_model, query, retrieved_documents)\n",
    "    print(\"Reranked documents:\")\n",
    "    print(\"--\" * 50)\n",
    "    print(len(reranked_documents),reranked_documents)\n",
    "    print(\"Reranked document:\")\n",
    "    print(\"--\" * 50)\n",
    "    print(reranked_documents[0][0].page_content)\n",
    "    print(\"--\" * 50)\n",
    "    print(\"metadata:\", reranked_documents[0][0].metadata)\n",
    "    print(\"==\" * 50, \"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
